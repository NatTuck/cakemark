//
// Generated by LLVM NVPTX Back-End
//

.version 3.1
.target sm_20
.address_size 32

	.weak	_Z6mul_hill

.func  (.param .b64 func_retval0) _Z6mul_hill(
	.param .b64 _Z6mul_hill_param_0,
	.param .b64 _Z6mul_hill_param_1
)
{
	.reg .s64 	%rl<19>;

	ld.param.u64 	%rl1, [_Z6mul_hill_param_0];
	shr.s64 	%rl3, %rl1, 32;
	and.b64  	%rl4, %rl1, 4294967295;
	ld.param.u64 	%rl2, [_Z6mul_hill_param_1];
	shr.s64 	%rl5, %rl2, 32;
	and.b64  	%rl6, %rl2, 4294967295;
	mul.lo.s64 	%rl7, %rl6, %rl3;
	mul.lo.s64 	%rl8, %rl6, %rl4;
	shr.u64 	%rl9, %rl8, 32;
	mad.lo.s64 	%rl10, %rl5, %rl4, %rl9;
	shr.s64 	%rl11, %rl7, 1;
	shr.s64 	%rl12, %rl10, 1;
	add.s64 	%rl13, %rl12, %rl11;
	and.b64  	%rl14, %rl7, %rl10;
	and.b64  	%rl15, %rl14, 1;
	add.s64 	%rl16, %rl13, %rl15;
	shr.s64 	%rl17, %rl16, 31;
	mad.lo.s64 	%rl18, %rl5, %rl3, %rl17;
	st.param.b64	[func_retval0+0], %rl18;
	ret;
}

	.weak	_Z6mul_himm
.func  (.param .b64 func_retval0) _Z6mul_himm(
	.param .b64 _Z6mul_himm_param_0,
	.param .b64 _Z6mul_himm_param_1
)
{
	.reg .s64 	%rl<19>;

	ld.param.u64 	%rl1, [_Z6mul_himm_param_0];
	ld.param.u64 	%rl2, [_Z6mul_himm_param_1];
	shr.u64 	%rl3, %rl1, 32;
	and.b64  	%rl4, %rl1, 4294967295;
	shr.u64 	%rl5, %rl2, 32;
	and.b64  	%rl6, %rl2, 4294967295;
	mul.lo.s64 	%rl7, %rl6, %rl3;
	mul.lo.s64 	%rl8, %rl6, %rl4;
	shr.u64 	%rl9, %rl8, 32;
	mad.lo.s64 	%rl10, %rl5, %rl4, %rl9;
	shr.u64 	%rl11, %rl7, 1;
	shr.u64 	%rl12, %rl10, 1;
	add.s64 	%rl13, %rl12, %rl11;
	and.b64  	%rl14, %rl7, %rl10;
	and.b64  	%rl15, %rl14, 1;
	add.s64 	%rl16, %rl13, %rl15;
	shr.u64 	%rl17, %rl16, 31;
	mad.lo.s64 	%rl18, %rl5, %rl3, %rl17;
	st.param.b64	[func_retval0+0], %rl18;
	ret;
}

	.weak	_Z6mul_hicc
.func  (.param .b32 func_retval0) _Z6mul_hicc(
	.param .b32 _Z6mul_hicc_param_0,
	.param .b32 _Z6mul_hicc_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<8>;

	ld.param.u8 	%rs1, [_Z6mul_hicc_param_0];
	ld.param.u8 	%rs2, [_Z6mul_hicc_param_1];
	cvt.u32.u16	%r1, %rs2;
	cvt.u32.u16	%r2, %rs1;
	cvt.s32.s8 	%r3, %r2;
	cvt.s32.s8 	%r4, %r1;
	mul.lo.s32 	%r5, %r4, %r3;
	shr.u32 	%r6, %r5, 8;
	and.b32  	%r7, %r6, 255;
	st.param.b32	[func_retval0+0], %r7;
	ret;
}

	.weak	_Z6mul_hiDv2_cS_
.func  (.param .align 2 .b8 func_retval0[2]) _Z6mul_hiDv2_cS_(
	.param .align 2 .b8 _Z6mul_hiDv2_cS__param_0[2],
	.param .align 2 .b8 _Z6mul_hiDv2_cS__param_1[2]
)
{
	.reg .s16 	%rs<7>;
	.reg .s32 	%r<13>;

	ld.param.v2.u8 	{%rs1, %rs2}, [_Z6mul_hiDv2_cS__param_0];
	ld.param.v2.u8 	{%rs3, %rs4}, [_Z6mul_hiDv2_cS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.s32.s8 	%r2, %r1;
	cvt.u32.u16	%r3, %rs3;
	cvt.s32.s8 	%r4, %r3;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u32.u16	%r7, %rs2;
	cvt.s32.s8 	%r8, %r7;
	cvt.u32.u16	%r9, %rs4;
	cvt.s32.s8 	%r10, %r9;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs5, %r12;
	cvt.u16.u32	%rs6, %r6;
	st.param.v2.b8	[func_retval0+0], {%rs6, %rs5};
	ret;
}

	.weak	_Z6mul_hiDv3_cS_
.func  (.param .align 4 .b8 func_retval0[3]) _Z6mul_hiDv3_cS_(
	.param .align 4 .b8 _Z6mul_hiDv3_cS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv3_cS__param_1[4]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<19>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs7}, [_Z6mul_hiDv3_cS__param_0];
	ld.param.v4.u8 	{%rs4, %rs5, %rs6, %rs8}, [_Z6mul_hiDv3_cS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.s32.s8 	%r2, %r1;
	cvt.u32.u16	%r3, %rs4;
	cvt.s32.s8 	%r4, %r3;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u32.u16	%r7, %rs2;
	cvt.s32.s8 	%r8, %r7;
	cvt.u32.u16	%r9, %rs5;
	cvt.s32.s8 	%r10, %r9;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u32.u16	%r13, %rs3;
	cvt.s32.s8 	%r14, %r13;
	cvt.u32.u16	%r15, %rs6;
	cvt.s32.s8 	%r16, %r15;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs9, %r18;
	cvt.u16.u32	%rs10, %r12;
	cvt.u16.u32	%rs11, %r6;
	st.param.v4.b8	[func_retval0+0], {%rs11, %rs10, %rs9, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv4_cS_
.func  (.param .align 4 .b8 func_retval0[4]) _Z6mul_hiDv4_cS_(
	.param .align 4 .b8 _Z6mul_hiDv4_cS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv4_cS__param_1[4]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<25>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv4_cS__param_0];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv4_cS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.s32.s8 	%r2, %r1;
	cvt.u32.u16	%r3, %rs5;
	cvt.s32.s8 	%r4, %r3;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs9, %r6;
	cvt.u32.u16	%r7, %rs2;
	cvt.s32.s8 	%r8, %r7;
	cvt.u32.u16	%r9, %rs6;
	cvt.s32.s8 	%r10, %r9;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs10, %r12;
	cvt.u32.u16	%r13, %rs3;
	cvt.s32.s8 	%r14, %r13;
	cvt.u32.u16	%r15, %rs7;
	cvt.s32.s8 	%r16, %r15;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs11, %r18;
	cvt.u32.u16	%r19, %rs4;
	cvt.s32.s8 	%r20, %r19;
	cvt.u32.u16	%r21, %rs8;
	cvt.s32.s8 	%r22, %r21;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs12, %r24;
	st.param.v4.b8	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv8_cS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv8_cS_(
	.param .align 8 .b8 _Z6mul_hiDv8_cS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv8_cS__param_1[8]
)
{
	.reg .s16 	%rs<25>;
	.reg .s32 	%r<49>;

	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv8_cS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv8_cS__param_0];
	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv8_cS__param_1+4];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv8_cS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.s32.s8 	%r2, %r1;
	cvt.u32.u16	%r3, %rs9;
	cvt.s32.s8 	%r4, %r3;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs17, %r6;
	cvt.u32.u16	%r7, %rs2;
	cvt.s32.s8 	%r8, %r7;
	cvt.u32.u16	%r9, %rs10;
	cvt.s32.s8 	%r10, %r9;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs18, %r12;
	cvt.u32.u16	%r13, %rs3;
	cvt.s32.s8 	%r14, %r13;
	cvt.u32.u16	%r15, %rs11;
	cvt.s32.s8 	%r16, %r15;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs19, %r18;
	cvt.u32.u16	%r19, %rs4;
	cvt.s32.s8 	%r20, %r19;
	cvt.u32.u16	%r21, %rs12;
	cvt.s32.s8 	%r22, %r21;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs20, %r24;
	cvt.u32.u16	%r25, %rs5;
	cvt.s32.s8 	%r26, %r25;
	cvt.u32.u16	%r27, %rs13;
	cvt.s32.s8 	%r28, %r27;
	mul.lo.s32 	%r29, %r28, %r26;
	shr.u32 	%r30, %r29, 8;
	cvt.u16.u32	%rs21, %r30;
	cvt.u32.u16	%r31, %rs6;
	cvt.s32.s8 	%r32, %r31;
	cvt.u32.u16	%r33, %rs14;
	cvt.s32.s8 	%r34, %r33;
	mul.lo.s32 	%r35, %r34, %r32;
	shr.u32 	%r36, %r35, 8;
	cvt.u16.u32	%rs22, %r36;
	cvt.u32.u16	%r37, %rs7;
	cvt.s32.s8 	%r38, %r37;
	cvt.u32.u16	%r39, %rs15;
	cvt.s32.s8 	%r40, %r39;
	mul.lo.s32 	%r41, %r40, %r38;
	shr.u32 	%r42, %r41, 8;
	cvt.u16.u32	%rs23, %r42;
	cvt.u32.u16	%r43, %rs8;
	cvt.s32.s8 	%r44, %r43;
	cvt.u32.u16	%r45, %rs16;
	cvt.s32.s8 	%r46, %r45;
	mul.lo.s32 	%r47, %r46, %r44;
	shr.u32 	%r48, %r47, 8;
	cvt.u16.u32	%rs24, %r48;
	st.param.v4.b8	[func_retval0+0], {%rs17, %rs18, %rs19, %rs20};
	st.param.v4.b8	[func_retval0+8], {%rs21, %rs22, %rs23, %rs24};
	ret;
}

	.weak	_Z6mul_hiDv16_cS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv16_cS_(
	.param .align 16 .b8 _Z6mul_hiDv16_cS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv16_cS__param_1[16]
)
{
	.reg .s16 	%rs<49>;
	.reg .s32 	%r<97>;

	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv16_cS__param_0+12];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv16_cS__param_0+8];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv16_cS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv16_cS__param_0];
	ld.param.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [_Z6mul_hiDv16_cS__param_1+12];
	ld.param.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [_Z6mul_hiDv16_cS__param_1+8];
	ld.param.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [_Z6mul_hiDv16_cS__param_1+4];
	ld.param.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [_Z6mul_hiDv16_cS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.s32.s8 	%r2, %r1;
	cvt.u32.u16	%r3, %rs17;
	cvt.s32.s8 	%r4, %r3;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs33, %r6;
	cvt.u32.u16	%r7, %rs2;
	cvt.s32.s8 	%r8, %r7;
	cvt.u32.u16	%r9, %rs18;
	cvt.s32.s8 	%r10, %r9;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs34, %r12;
	cvt.u32.u16	%r13, %rs3;
	cvt.s32.s8 	%r14, %r13;
	cvt.u32.u16	%r15, %rs19;
	cvt.s32.s8 	%r16, %r15;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs35, %r18;
	cvt.u32.u16	%r19, %rs4;
	cvt.s32.s8 	%r20, %r19;
	cvt.u32.u16	%r21, %rs20;
	cvt.s32.s8 	%r22, %r21;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs36, %r24;
	cvt.u32.u16	%r25, %rs5;
	cvt.s32.s8 	%r26, %r25;
	cvt.u32.u16	%r27, %rs21;
	cvt.s32.s8 	%r28, %r27;
	mul.lo.s32 	%r29, %r28, %r26;
	shr.u32 	%r30, %r29, 8;
	cvt.u16.u32	%rs37, %r30;
	cvt.u32.u16	%r31, %rs6;
	cvt.s32.s8 	%r32, %r31;
	cvt.u32.u16	%r33, %rs22;
	cvt.s32.s8 	%r34, %r33;
	mul.lo.s32 	%r35, %r34, %r32;
	shr.u32 	%r36, %r35, 8;
	cvt.u16.u32	%rs38, %r36;
	cvt.u32.u16	%r37, %rs7;
	cvt.s32.s8 	%r38, %r37;
	cvt.u32.u16	%r39, %rs23;
	cvt.s32.s8 	%r40, %r39;
	mul.lo.s32 	%r41, %r40, %r38;
	shr.u32 	%r42, %r41, 8;
	cvt.u16.u32	%rs39, %r42;
	cvt.u32.u16	%r43, %rs8;
	cvt.s32.s8 	%r44, %r43;
	cvt.u32.u16	%r45, %rs24;
	cvt.s32.s8 	%r46, %r45;
	mul.lo.s32 	%r47, %r46, %r44;
	shr.u32 	%r48, %r47, 8;
	cvt.u16.u32	%rs40, %r48;
	cvt.u32.u16	%r49, %rs9;
	cvt.s32.s8 	%r50, %r49;
	cvt.u32.u16	%r51, %rs25;
	cvt.s32.s8 	%r52, %r51;
	mul.lo.s32 	%r53, %r52, %r50;
	shr.u32 	%r54, %r53, 8;
	cvt.u16.u32	%rs41, %r54;
	cvt.u32.u16	%r55, %rs10;
	cvt.s32.s8 	%r56, %r55;
	cvt.u32.u16	%r57, %rs26;
	cvt.s32.s8 	%r58, %r57;
	mul.lo.s32 	%r59, %r58, %r56;
	shr.u32 	%r60, %r59, 8;
	cvt.u16.u32	%rs42, %r60;
	cvt.u32.u16	%r61, %rs11;
	cvt.s32.s8 	%r62, %r61;
	cvt.u32.u16	%r63, %rs27;
	cvt.s32.s8 	%r64, %r63;
	mul.lo.s32 	%r65, %r64, %r62;
	shr.u32 	%r66, %r65, 8;
	cvt.u16.u32	%rs43, %r66;
	cvt.u32.u16	%r67, %rs12;
	cvt.s32.s8 	%r68, %r67;
	cvt.u32.u16	%r69, %rs28;
	cvt.s32.s8 	%r70, %r69;
	mul.lo.s32 	%r71, %r70, %r68;
	shr.u32 	%r72, %r71, 8;
	cvt.u16.u32	%rs44, %r72;
	cvt.u32.u16	%r73, %rs13;
	cvt.s32.s8 	%r74, %r73;
	cvt.u32.u16	%r75, %rs29;
	cvt.s32.s8 	%r76, %r75;
	mul.lo.s32 	%r77, %r76, %r74;
	shr.u32 	%r78, %r77, 8;
	cvt.u16.u32	%rs45, %r78;
	cvt.u32.u16	%r79, %rs14;
	cvt.s32.s8 	%r80, %r79;
	cvt.u32.u16	%r81, %rs30;
	cvt.s32.s8 	%r82, %r81;
	mul.lo.s32 	%r83, %r82, %r80;
	shr.u32 	%r84, %r83, 8;
	cvt.u16.u32	%rs46, %r84;
	cvt.u32.u16	%r85, %rs15;
	cvt.s32.s8 	%r86, %r85;
	cvt.u32.u16	%r87, %rs31;
	cvt.s32.s8 	%r88, %r87;
	mul.lo.s32 	%r89, %r88, %r86;
	shr.u32 	%r90, %r89, 8;
	cvt.u16.u32	%rs47, %r90;
	cvt.u32.u16	%r91, %rs16;
	cvt.s32.s8 	%r92, %r91;
	cvt.u32.u16	%r93, %rs32;
	cvt.s32.s8 	%r94, %r93;
	mul.lo.s32 	%r95, %r94, %r92;
	shr.u32 	%r96, %r95, 8;
	cvt.u16.u32	%rs48, %r96;
	st.param.v4.b8	[func_retval0+0], {%rs33, %rs34, %rs35, %rs36};
	st.param.v4.b8	[func_retval0+8], {%rs37, %rs38, %rs39, %rs40};
	st.param.v4.b8	[func_retval0+16], {%rs41, %rs42, %rs43, %rs44};
	st.param.v4.b8	[func_retval0+24], {%rs45, %rs46, %rs47, %rs48};
	ret;
}

	.weak	_Z6mul_hihh
.func  (.param .b32 func_retval0) _Z6mul_hihh(
	.param .b32 _Z6mul_hihh_param_0,
	.param .b32 _Z6mul_hihh_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;

	ld.param.u8 	%rs1, [_Z6mul_hihh_param_0];
	ld.param.u8 	%rs2, [_Z6mul_hihh_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 8;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	.weak	_Z6mul_hiDv2_hS_
.func  (.param .align 2 .b8 func_retval0[2]) _Z6mul_hiDv2_hS_(
	.param .align 2 .b8 _Z6mul_hiDv2_hS__param_0[2],
	.param .align 2 .b8 _Z6mul_hiDv2_hS__param_1[2]
)
{
	.reg .s16 	%rs<7>;
	.reg .s32 	%r<13>;

	ld.param.v2.u8 	{%rs1, %rs2}, [_Z6mul_hiDv2_hS__param_0];
	ld.param.v2.u8 	{%rs3, %rs4}, [_Z6mul_hiDv2_hS__param_1];
	cvt.u32.u16	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	cvt.u32.u16	%r3, %rs3;
	and.b32  	%r4, %r3, 255;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u32.u16	%r7, %rs2;
	and.b32  	%r8, %r7, 255;
	cvt.u32.u16	%r9, %rs4;
	and.b32  	%r10, %r9, 255;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs5, %r12;
	cvt.u16.u32	%rs6, %r6;
	st.param.v2.b8	[func_retval0+0], {%rs6, %rs5};
	ret;
}

	.weak	_Z6mul_hiDv3_hS_
.func  (.param .align 4 .b8 func_retval0[3]) _Z6mul_hiDv3_hS_(
	.param .align 4 .b8 _Z6mul_hiDv3_hS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv3_hS__param_1[4]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<19>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs7}, [_Z6mul_hiDv3_hS__param_0];
	ld.param.v4.u8 	{%rs4, %rs5, %rs6, %rs8}, [_Z6mul_hiDv3_hS__param_1];
	cvt.u32.u16	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	cvt.u32.u16	%r3, %rs4;
	and.b32  	%r4, %r3, 255;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u32.u16	%r7, %rs2;
	and.b32  	%r8, %r7, 255;
	cvt.u32.u16	%r9, %rs5;
	and.b32  	%r10, %r9, 255;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u32.u16	%r13, %rs3;
	and.b32  	%r14, %r13, 255;
	cvt.u32.u16	%r15, %rs6;
	and.b32  	%r16, %r15, 255;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs9, %r18;
	cvt.u16.u32	%rs10, %r12;
	cvt.u16.u32	%rs11, %r6;
	st.param.v4.b8	[func_retval0+0], {%rs11, %rs10, %rs9, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv4_hS_
.func  (.param .align 4 .b8 func_retval0[4]) _Z6mul_hiDv4_hS_(
	.param .align 4 .b8 _Z6mul_hiDv4_hS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv4_hS__param_1[4]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<25>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv4_hS__param_0];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv4_hS__param_1];
	cvt.u32.u16	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	cvt.u32.u16	%r3, %rs5;
	and.b32  	%r4, %r3, 255;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs9, %r6;
	cvt.u32.u16	%r7, %rs2;
	and.b32  	%r8, %r7, 255;
	cvt.u32.u16	%r9, %rs6;
	and.b32  	%r10, %r9, 255;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs10, %r12;
	cvt.u32.u16	%r13, %rs3;
	and.b32  	%r14, %r13, 255;
	cvt.u32.u16	%r15, %rs7;
	and.b32  	%r16, %r15, 255;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs11, %r18;
	cvt.u32.u16	%r19, %rs4;
	and.b32  	%r20, %r19, 255;
	cvt.u32.u16	%r21, %rs8;
	and.b32  	%r22, %r21, 255;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs12, %r24;
	st.param.v4.b8	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv8_hS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv8_hS_(
	.param .align 8 .b8 _Z6mul_hiDv8_hS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv8_hS__param_1[8]
)
{
	.reg .s16 	%rs<25>;
	.reg .s32 	%r<49>;

	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv8_hS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv8_hS__param_0];
	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv8_hS__param_1+4];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv8_hS__param_1];
	cvt.u32.u16	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	cvt.u32.u16	%r3, %rs9;
	and.b32  	%r4, %r3, 255;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs17, %r6;
	cvt.u32.u16	%r7, %rs2;
	and.b32  	%r8, %r7, 255;
	cvt.u32.u16	%r9, %rs10;
	and.b32  	%r10, %r9, 255;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs18, %r12;
	cvt.u32.u16	%r13, %rs3;
	and.b32  	%r14, %r13, 255;
	cvt.u32.u16	%r15, %rs11;
	and.b32  	%r16, %r15, 255;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs19, %r18;
	cvt.u32.u16	%r19, %rs4;
	and.b32  	%r20, %r19, 255;
	cvt.u32.u16	%r21, %rs12;
	and.b32  	%r22, %r21, 255;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs20, %r24;
	cvt.u32.u16	%r25, %rs5;
	and.b32  	%r26, %r25, 255;
	cvt.u32.u16	%r27, %rs13;
	and.b32  	%r28, %r27, 255;
	mul.lo.s32 	%r29, %r28, %r26;
	shr.u32 	%r30, %r29, 8;
	cvt.u16.u32	%rs21, %r30;
	cvt.u32.u16	%r31, %rs6;
	and.b32  	%r32, %r31, 255;
	cvt.u32.u16	%r33, %rs14;
	and.b32  	%r34, %r33, 255;
	mul.lo.s32 	%r35, %r34, %r32;
	shr.u32 	%r36, %r35, 8;
	cvt.u16.u32	%rs22, %r36;
	cvt.u32.u16	%r37, %rs7;
	and.b32  	%r38, %r37, 255;
	cvt.u32.u16	%r39, %rs15;
	and.b32  	%r40, %r39, 255;
	mul.lo.s32 	%r41, %r40, %r38;
	shr.u32 	%r42, %r41, 8;
	cvt.u16.u32	%rs23, %r42;
	cvt.u32.u16	%r43, %rs8;
	and.b32  	%r44, %r43, 255;
	cvt.u32.u16	%r45, %rs16;
	and.b32  	%r46, %r45, 255;
	mul.lo.s32 	%r47, %r46, %r44;
	shr.u32 	%r48, %r47, 8;
	cvt.u16.u32	%rs24, %r48;
	st.param.v4.b8	[func_retval0+0], {%rs17, %rs18, %rs19, %rs20};
	st.param.v4.b8	[func_retval0+8], {%rs21, %rs22, %rs23, %rs24};
	ret;
}

	.weak	_Z6mul_hiDv16_hS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv16_hS_(
	.param .align 16 .b8 _Z6mul_hiDv16_hS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv16_hS__param_1[16]
)
{
	.reg .s16 	%rs<49>;
	.reg .s32 	%r<97>;

	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv16_hS__param_0+12];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv16_hS__param_0+8];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv16_hS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv16_hS__param_0];
	ld.param.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [_Z6mul_hiDv16_hS__param_1+12];
	ld.param.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [_Z6mul_hiDv16_hS__param_1+8];
	ld.param.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [_Z6mul_hiDv16_hS__param_1+4];
	ld.param.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [_Z6mul_hiDv16_hS__param_1];
	cvt.u32.u16	%r1, %rs1;
	and.b32  	%r2, %r1, 255;
	cvt.u32.u16	%r3, %rs17;
	and.b32  	%r4, %r3, 255;
	mul.lo.s32 	%r5, %r4, %r2;
	shr.u32 	%r6, %r5, 8;
	cvt.u16.u32	%rs33, %r6;
	cvt.u32.u16	%r7, %rs2;
	and.b32  	%r8, %r7, 255;
	cvt.u32.u16	%r9, %rs18;
	and.b32  	%r10, %r9, 255;
	mul.lo.s32 	%r11, %r10, %r8;
	shr.u32 	%r12, %r11, 8;
	cvt.u16.u32	%rs34, %r12;
	cvt.u32.u16	%r13, %rs3;
	and.b32  	%r14, %r13, 255;
	cvt.u32.u16	%r15, %rs19;
	and.b32  	%r16, %r15, 255;
	mul.lo.s32 	%r17, %r16, %r14;
	shr.u32 	%r18, %r17, 8;
	cvt.u16.u32	%rs35, %r18;
	cvt.u32.u16	%r19, %rs4;
	and.b32  	%r20, %r19, 255;
	cvt.u32.u16	%r21, %rs20;
	and.b32  	%r22, %r21, 255;
	mul.lo.s32 	%r23, %r22, %r20;
	shr.u32 	%r24, %r23, 8;
	cvt.u16.u32	%rs36, %r24;
	cvt.u32.u16	%r25, %rs5;
	and.b32  	%r26, %r25, 255;
	cvt.u32.u16	%r27, %rs21;
	and.b32  	%r28, %r27, 255;
	mul.lo.s32 	%r29, %r28, %r26;
	shr.u32 	%r30, %r29, 8;
	cvt.u16.u32	%rs37, %r30;
	cvt.u32.u16	%r31, %rs6;
	and.b32  	%r32, %r31, 255;
	cvt.u32.u16	%r33, %rs22;
	and.b32  	%r34, %r33, 255;
	mul.lo.s32 	%r35, %r34, %r32;
	shr.u32 	%r36, %r35, 8;
	cvt.u16.u32	%rs38, %r36;
	cvt.u32.u16	%r37, %rs7;
	and.b32  	%r38, %r37, 255;
	cvt.u32.u16	%r39, %rs23;
	and.b32  	%r40, %r39, 255;
	mul.lo.s32 	%r41, %r40, %r38;
	shr.u32 	%r42, %r41, 8;
	cvt.u16.u32	%rs39, %r42;
	cvt.u32.u16	%r43, %rs8;
	and.b32  	%r44, %r43, 255;
	cvt.u32.u16	%r45, %rs24;
	and.b32  	%r46, %r45, 255;
	mul.lo.s32 	%r47, %r46, %r44;
	shr.u32 	%r48, %r47, 8;
	cvt.u16.u32	%rs40, %r48;
	cvt.u32.u16	%r49, %rs9;
	and.b32  	%r50, %r49, 255;
	cvt.u32.u16	%r51, %rs25;
	and.b32  	%r52, %r51, 255;
	mul.lo.s32 	%r53, %r52, %r50;
	shr.u32 	%r54, %r53, 8;
	cvt.u16.u32	%rs41, %r54;
	cvt.u32.u16	%r55, %rs10;
	and.b32  	%r56, %r55, 255;
	cvt.u32.u16	%r57, %rs26;
	and.b32  	%r58, %r57, 255;
	mul.lo.s32 	%r59, %r58, %r56;
	shr.u32 	%r60, %r59, 8;
	cvt.u16.u32	%rs42, %r60;
	cvt.u32.u16	%r61, %rs11;
	and.b32  	%r62, %r61, 255;
	cvt.u32.u16	%r63, %rs27;
	and.b32  	%r64, %r63, 255;
	mul.lo.s32 	%r65, %r64, %r62;
	shr.u32 	%r66, %r65, 8;
	cvt.u16.u32	%rs43, %r66;
	cvt.u32.u16	%r67, %rs12;
	and.b32  	%r68, %r67, 255;
	cvt.u32.u16	%r69, %rs28;
	and.b32  	%r70, %r69, 255;
	mul.lo.s32 	%r71, %r70, %r68;
	shr.u32 	%r72, %r71, 8;
	cvt.u16.u32	%rs44, %r72;
	cvt.u32.u16	%r73, %rs13;
	and.b32  	%r74, %r73, 255;
	cvt.u32.u16	%r75, %rs29;
	and.b32  	%r76, %r75, 255;
	mul.lo.s32 	%r77, %r76, %r74;
	shr.u32 	%r78, %r77, 8;
	cvt.u16.u32	%rs45, %r78;
	cvt.u32.u16	%r79, %rs14;
	and.b32  	%r80, %r79, 255;
	cvt.u32.u16	%r81, %rs30;
	and.b32  	%r82, %r81, 255;
	mul.lo.s32 	%r83, %r82, %r80;
	shr.u32 	%r84, %r83, 8;
	cvt.u16.u32	%rs46, %r84;
	cvt.u32.u16	%r85, %rs15;
	and.b32  	%r86, %r85, 255;
	cvt.u32.u16	%r87, %rs31;
	and.b32  	%r88, %r87, 255;
	mul.lo.s32 	%r89, %r88, %r86;
	shr.u32 	%r90, %r89, 8;
	cvt.u16.u32	%rs47, %r90;
	cvt.u32.u16	%r91, %rs16;
	and.b32  	%r92, %r91, 255;
	cvt.u32.u16	%r93, %rs32;
	and.b32  	%r94, %r93, 255;
	mul.lo.s32 	%r95, %r94, %r92;
	shr.u32 	%r96, %r95, 8;
	cvt.u16.u32	%rs48, %r96;
	st.param.v4.b8	[func_retval0+0], {%rs33, %rs34, %rs35, %rs36};
	st.param.v4.b8	[func_retval0+8], {%rs37, %rs38, %rs39, %rs40};
	st.param.v4.b8	[func_retval0+16], {%rs41, %rs42, %rs43, %rs44};
	st.param.v4.b8	[func_retval0+24], {%rs45, %rs46, %rs47, %rs48};
	ret;
}

	.weak	_Z6mul_hiss
.func  (.param .b32 func_retval0) _Z6mul_hiss(
	.param .b32 _Z6mul_hiss_param_0,
	.param .b32 _Z6mul_hiss_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;

	ld.param.u16 	%rs1, [_Z6mul_hiss_param_0];
	ld.param.u16 	%rs2, [_Z6mul_hiss_param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs2;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	.weak	_Z6mul_hiDv2_sS_
.func  (.param .align 4 .b8 func_retval0[4]) _Z6mul_hiDv2_sS_(
	.param .align 4 .b8 _Z6mul_hiDv2_sS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv2_sS__param_1[4]
)
{
	.reg .s16 	%rs<7>;
	.reg .s32 	%r<9>;

	ld.param.v2.u16 	{%rs1, %rs2}, [_Z6mul_hiDv2_sS__param_0];
	ld.param.v2.u16 	{%rs3, %rs4}, [_Z6mul_hiDv2_sS__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs3;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs5, %r4;
	cvt.s32.s16	%r5, %rs2;
	cvt.s32.s16	%r6, %rs4;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs6, %r8;
	st.param.v2.b16	[func_retval0+0], {%rs5, %rs6};
	ret;
}

	.weak	_Z6mul_hiDv3_sS_
.func  (.param .align 8 .b8 func_retval0[6]) _Z6mul_hiDv3_sS_(
	.param .align 8 .b8 _Z6mul_hiDv3_sS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv3_sS__param_1[8]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<13>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs7}, [_Z6mul_hiDv3_sS__param_0];
	ld.param.v4.u16 	{%rs4, %rs5, %rs6, %rs8}, [_Z6mul_hiDv3_sS__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs4;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs9, %r4;
	cvt.s32.s16	%r5, %rs2;
	cvt.s32.s16	%r6, %rs5;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs10, %r8;
	cvt.s32.s16	%r9, %rs3;
	cvt.s32.s16	%r10, %rs6;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs11, %r12;
	st.param.v4.b16	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv4_sS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv4_sS_(
	.param .align 8 .b8 _Z6mul_hiDv4_sS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv4_sS__param_1[8]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<17>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv4_sS__param_0];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv4_sS__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs5;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs9, %r4;
	cvt.s32.s16	%r5, %rs2;
	cvt.s32.s16	%r6, %rs6;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs10, %r8;
	cvt.s32.s16	%r9, %rs3;
	cvt.s32.s16	%r10, %rs7;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs11, %r12;
	cvt.s32.s16	%r13, %rs4;
	cvt.s32.s16	%r14, %rs8;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs12, %r16;
	st.param.v4.b16	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv8_sS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv8_sS_(
	.param .align 16 .b8 _Z6mul_hiDv8_sS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv8_sS__param_1[16]
)
{
	.reg .s16 	%rs<25>;
	.reg .s32 	%r<33>;

	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv8_sS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv8_sS__param_0];
	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv8_sS__param_1+8];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv8_sS__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs9;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs17, %r4;
	cvt.s32.s16	%r5, %rs2;
	cvt.s32.s16	%r6, %rs10;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs18, %r8;
	cvt.s32.s16	%r9, %rs3;
	cvt.s32.s16	%r10, %rs11;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs19, %r12;
	cvt.s32.s16	%r13, %rs4;
	cvt.s32.s16	%r14, %rs12;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs20, %r16;
	cvt.s32.s16	%r17, %rs5;
	cvt.s32.s16	%r18, %rs13;
	mul.lo.s32 	%r19, %r18, %r17;
	shr.u32 	%r20, %r19, 16;
	cvt.u16.u32	%rs21, %r20;
	cvt.s32.s16	%r21, %rs6;
	cvt.s32.s16	%r22, %rs14;
	mul.lo.s32 	%r23, %r22, %r21;
	shr.u32 	%r24, %r23, 16;
	cvt.u16.u32	%rs22, %r24;
	cvt.s32.s16	%r25, %rs7;
	cvt.s32.s16	%r26, %rs15;
	mul.lo.s32 	%r27, %r26, %r25;
	shr.u32 	%r28, %r27, 16;
	cvt.u16.u32	%rs23, %r28;
	cvt.s32.s16	%r29, %rs8;
	cvt.s32.s16	%r30, %rs16;
	mul.lo.s32 	%r31, %r30, %r29;
	shr.u32 	%r32, %r31, 16;
	cvt.u16.u32	%rs24, %r32;
	st.param.v4.b16	[func_retval0+0], {%rs17, %rs18, %rs19, %rs20};
	st.param.v4.b16	[func_retval0+8], {%rs21, %rs22, %rs23, %rs24};
	ret;
}

	.weak	_Z6mul_hiDv16_sS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv16_sS_(
	.param .align 32 .b8 _Z6mul_hiDv16_sS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv16_sS__param_1[32]
)
{
	.reg .s16 	%rs<49>;
	.reg .s32 	%r<65>;

	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv16_sS__param_0+24];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv16_sS__param_0+16];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv16_sS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv16_sS__param_0];
	ld.param.v4.u16 	{%rs29, %rs30, %rs31, %rs32}, [_Z6mul_hiDv16_sS__param_1+24];
	ld.param.v4.u16 	{%rs25, %rs26, %rs27, %rs28}, [_Z6mul_hiDv16_sS__param_1+16];
	ld.param.v4.u16 	{%rs21, %rs22, %rs23, %rs24}, [_Z6mul_hiDv16_sS__param_1+8];
	ld.param.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [_Z6mul_hiDv16_sS__param_1];
	cvt.s32.s16	%r1, %rs1;
	cvt.s32.s16	%r2, %rs17;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs33, %r4;
	cvt.s32.s16	%r5, %rs2;
	cvt.s32.s16	%r6, %rs18;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs34, %r8;
	cvt.s32.s16	%r9, %rs3;
	cvt.s32.s16	%r10, %rs19;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs35, %r12;
	cvt.s32.s16	%r13, %rs4;
	cvt.s32.s16	%r14, %rs20;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs36, %r16;
	cvt.s32.s16	%r17, %rs5;
	cvt.s32.s16	%r18, %rs21;
	mul.lo.s32 	%r19, %r18, %r17;
	shr.u32 	%r20, %r19, 16;
	cvt.u16.u32	%rs37, %r20;
	cvt.s32.s16	%r21, %rs6;
	cvt.s32.s16	%r22, %rs22;
	mul.lo.s32 	%r23, %r22, %r21;
	shr.u32 	%r24, %r23, 16;
	cvt.u16.u32	%rs38, %r24;
	cvt.s32.s16	%r25, %rs7;
	cvt.s32.s16	%r26, %rs23;
	mul.lo.s32 	%r27, %r26, %r25;
	shr.u32 	%r28, %r27, 16;
	cvt.u16.u32	%rs39, %r28;
	cvt.s32.s16	%r29, %rs8;
	cvt.s32.s16	%r30, %rs24;
	mul.lo.s32 	%r31, %r30, %r29;
	shr.u32 	%r32, %r31, 16;
	cvt.u16.u32	%rs40, %r32;
	cvt.s32.s16	%r33, %rs9;
	cvt.s32.s16	%r34, %rs25;
	mul.lo.s32 	%r35, %r34, %r33;
	shr.u32 	%r36, %r35, 16;
	cvt.u16.u32	%rs41, %r36;
	cvt.s32.s16	%r37, %rs10;
	cvt.s32.s16	%r38, %rs26;
	mul.lo.s32 	%r39, %r38, %r37;
	shr.u32 	%r40, %r39, 16;
	cvt.u16.u32	%rs42, %r40;
	cvt.s32.s16	%r41, %rs11;
	cvt.s32.s16	%r42, %rs27;
	mul.lo.s32 	%r43, %r42, %r41;
	shr.u32 	%r44, %r43, 16;
	cvt.u16.u32	%rs43, %r44;
	cvt.s32.s16	%r45, %rs12;
	cvt.s32.s16	%r46, %rs28;
	mul.lo.s32 	%r47, %r46, %r45;
	shr.u32 	%r48, %r47, 16;
	cvt.u16.u32	%rs44, %r48;
	cvt.s32.s16	%r49, %rs13;
	cvt.s32.s16	%r50, %rs29;
	mul.lo.s32 	%r51, %r50, %r49;
	shr.u32 	%r52, %r51, 16;
	cvt.u16.u32	%rs45, %r52;
	cvt.s32.s16	%r53, %rs14;
	cvt.s32.s16	%r54, %rs30;
	mul.lo.s32 	%r55, %r54, %r53;
	shr.u32 	%r56, %r55, 16;
	cvt.u16.u32	%rs46, %r56;
	cvt.s32.s16	%r57, %rs15;
	cvt.s32.s16	%r58, %rs31;
	mul.lo.s32 	%r59, %r58, %r57;
	shr.u32 	%r60, %r59, 16;
	cvt.u16.u32	%rs47, %r60;
	cvt.s32.s16	%r61, %rs16;
	cvt.s32.s16	%r62, %rs32;
	mul.lo.s32 	%r63, %r62, %r61;
	shr.u32 	%r64, %r63, 16;
	cvt.u16.u32	%rs48, %r64;
	st.param.v4.b16	[func_retval0+0], {%rs33, %rs34, %rs35, %rs36};
	st.param.v4.b16	[func_retval0+8], {%rs37, %rs38, %rs39, %rs40};
	st.param.v4.b16	[func_retval0+16], {%rs41, %rs42, %rs43, %rs44};
	st.param.v4.b16	[func_retval0+24], {%rs45, %rs46, %rs47, %rs48};
	ret;
}

	.weak	_Z6mul_hitt
.func  (.param .b32 func_retval0) _Z6mul_hitt(
	.param .b32 _Z6mul_hitt_param_0,
	.param .b32 _Z6mul_hitt_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;

	ld.param.u16 	%rs1, [_Z6mul_hitt_param_0];
	ld.param.u16 	%rs2, [_Z6mul_hitt_param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs2;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	.weak	_Z6mul_hiDv2_tS_
.func  (.param .align 4 .b8 func_retval0[4]) _Z6mul_hiDv2_tS_(
	.param .align 4 .b8 _Z6mul_hiDv2_tS__param_0[4],
	.param .align 4 .b8 _Z6mul_hiDv2_tS__param_1[4]
)
{
	.reg .s16 	%rs<7>;
	.reg .s32 	%r<9>;

	ld.param.v2.u16 	{%rs1, %rs2}, [_Z6mul_hiDv2_tS__param_0];
	ld.param.v2.u16 	{%rs3, %rs4}, [_Z6mul_hiDv2_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs3;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs5, %r4;
	cvt.u32.u16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs4;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs6, %r8;
	st.param.v2.b16	[func_retval0+0], {%rs5, %rs6};
	ret;
}

	.weak	_Z6mul_hiDv3_tS_
.func  (.param .align 8 .b8 func_retval0[6]) _Z6mul_hiDv3_tS_(
	.param .align 8 .b8 _Z6mul_hiDv3_tS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv3_tS__param_1[8]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<13>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs7}, [_Z6mul_hiDv3_tS__param_0];
	ld.param.v4.u16 	{%rs4, %rs5, %rs6, %rs8}, [_Z6mul_hiDv3_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs4;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs9, %r4;
	cvt.u32.u16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs5;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs10, %r8;
	cvt.u32.u16	%r9, %rs3;
	cvt.u32.u16	%r10, %rs6;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs11, %r12;
	st.param.v4.b16	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv4_tS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv4_tS_(
	.param .align 8 .b8 _Z6mul_hiDv4_tS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv4_tS__param_1[8]
)
{
	.reg .s16 	%rs<13>;
	.reg .s32 	%r<17>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv4_tS__param_0];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv4_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs5;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs9, %r4;
	cvt.u32.u16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs6;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs10, %r8;
	cvt.u32.u16	%r9, %rs3;
	cvt.u32.u16	%r10, %rs7;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs11, %r12;
	cvt.u32.u16	%r13, %rs4;
	cvt.u32.u16	%r14, %rs8;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs12, %r16;
	st.param.v4.b16	[func_retval0+0], {%rs9, %rs10, %rs11, %rs12};
	ret;
}

	.weak	_Z6mul_hiDv8_tS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv8_tS_(
	.param .align 16 .b8 _Z6mul_hiDv8_tS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv8_tS__param_1[16]
)
{
	.reg .s16 	%rs<25>;
	.reg .s32 	%r<33>;

	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv8_tS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv8_tS__param_0];
	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv8_tS__param_1+8];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv8_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs9;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs17, %r4;
	cvt.u32.u16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs10;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs18, %r8;
	cvt.u32.u16	%r9, %rs3;
	cvt.u32.u16	%r10, %rs11;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs19, %r12;
	cvt.u32.u16	%r13, %rs4;
	cvt.u32.u16	%r14, %rs12;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs20, %r16;
	cvt.u32.u16	%r17, %rs5;
	cvt.u32.u16	%r18, %rs13;
	mul.lo.s32 	%r19, %r18, %r17;
	shr.u32 	%r20, %r19, 16;
	cvt.u16.u32	%rs21, %r20;
	cvt.u32.u16	%r21, %rs6;
	cvt.u32.u16	%r22, %rs14;
	mul.lo.s32 	%r23, %r22, %r21;
	shr.u32 	%r24, %r23, 16;
	cvt.u16.u32	%rs22, %r24;
	cvt.u32.u16	%r25, %rs7;
	cvt.u32.u16	%r26, %rs15;
	mul.lo.s32 	%r27, %r26, %r25;
	shr.u32 	%r28, %r27, 16;
	cvt.u16.u32	%rs23, %r28;
	cvt.u32.u16	%r29, %rs8;
	cvt.u32.u16	%r30, %rs16;
	mul.lo.s32 	%r31, %r30, %r29;
	shr.u32 	%r32, %r31, 16;
	cvt.u16.u32	%rs24, %r32;
	st.param.v4.b16	[func_retval0+0], {%rs17, %rs18, %rs19, %rs20};
	st.param.v4.b16	[func_retval0+8], {%rs21, %rs22, %rs23, %rs24};
	ret;
}

	.weak	_Z6mul_hiDv16_tS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv16_tS_(
	.param .align 32 .b8 _Z6mul_hiDv16_tS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv16_tS__param_1[32]
)
{
	.reg .s16 	%rs<49>;
	.reg .s32 	%r<65>;

	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z6mul_hiDv16_tS__param_0+24];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z6mul_hiDv16_tS__param_0+16];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z6mul_hiDv16_tS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z6mul_hiDv16_tS__param_0];
	ld.param.v4.u16 	{%rs29, %rs30, %rs31, %rs32}, [_Z6mul_hiDv16_tS__param_1+24];
	ld.param.v4.u16 	{%rs25, %rs26, %rs27, %rs28}, [_Z6mul_hiDv16_tS__param_1+16];
	ld.param.v4.u16 	{%rs21, %rs22, %rs23, %rs24}, [_Z6mul_hiDv16_tS__param_1+8];
	ld.param.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [_Z6mul_hiDv16_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	cvt.u32.u16	%r2, %rs17;
	mul.lo.s32 	%r3, %r2, %r1;
	shr.u32 	%r4, %r3, 16;
	cvt.u16.u32	%rs33, %r4;
	cvt.u32.u16	%r5, %rs2;
	cvt.u32.u16	%r6, %rs18;
	mul.lo.s32 	%r7, %r6, %r5;
	shr.u32 	%r8, %r7, 16;
	cvt.u16.u32	%rs34, %r8;
	cvt.u32.u16	%r9, %rs3;
	cvt.u32.u16	%r10, %rs19;
	mul.lo.s32 	%r11, %r10, %r9;
	shr.u32 	%r12, %r11, 16;
	cvt.u16.u32	%rs35, %r12;
	cvt.u32.u16	%r13, %rs4;
	cvt.u32.u16	%r14, %rs20;
	mul.lo.s32 	%r15, %r14, %r13;
	shr.u32 	%r16, %r15, 16;
	cvt.u16.u32	%rs36, %r16;
	cvt.u32.u16	%r17, %rs5;
	cvt.u32.u16	%r18, %rs21;
	mul.lo.s32 	%r19, %r18, %r17;
	shr.u32 	%r20, %r19, 16;
	cvt.u16.u32	%rs37, %r20;
	cvt.u32.u16	%r21, %rs6;
	cvt.u32.u16	%r22, %rs22;
	mul.lo.s32 	%r23, %r22, %r21;
	shr.u32 	%r24, %r23, 16;
	cvt.u16.u32	%rs38, %r24;
	cvt.u32.u16	%r25, %rs7;
	cvt.u32.u16	%r26, %rs23;
	mul.lo.s32 	%r27, %r26, %r25;
	shr.u32 	%r28, %r27, 16;
	cvt.u16.u32	%rs39, %r28;
	cvt.u32.u16	%r29, %rs8;
	cvt.u32.u16	%r30, %rs24;
	mul.lo.s32 	%r31, %r30, %r29;
	shr.u32 	%r32, %r31, 16;
	cvt.u16.u32	%rs40, %r32;
	cvt.u32.u16	%r33, %rs9;
	cvt.u32.u16	%r34, %rs25;
	mul.lo.s32 	%r35, %r34, %r33;
	shr.u32 	%r36, %r35, 16;
	cvt.u16.u32	%rs41, %r36;
	cvt.u32.u16	%r37, %rs10;
	cvt.u32.u16	%r38, %rs26;
	mul.lo.s32 	%r39, %r38, %r37;
	shr.u32 	%r40, %r39, 16;
	cvt.u16.u32	%rs42, %r40;
	cvt.u32.u16	%r41, %rs11;
	cvt.u32.u16	%r42, %rs27;
	mul.lo.s32 	%r43, %r42, %r41;
	shr.u32 	%r44, %r43, 16;
	cvt.u16.u32	%rs43, %r44;
	cvt.u32.u16	%r45, %rs12;
	cvt.u32.u16	%r46, %rs28;
	mul.lo.s32 	%r47, %r46, %r45;
	shr.u32 	%r48, %r47, 16;
	cvt.u16.u32	%rs44, %r48;
	cvt.u32.u16	%r49, %rs13;
	cvt.u32.u16	%r50, %rs29;
	mul.lo.s32 	%r51, %r50, %r49;
	shr.u32 	%r52, %r51, 16;
	cvt.u16.u32	%rs45, %r52;
	cvt.u32.u16	%r53, %rs14;
	cvt.u32.u16	%r54, %rs30;
	mul.lo.s32 	%r55, %r54, %r53;
	shr.u32 	%r56, %r55, 16;
	cvt.u16.u32	%rs46, %r56;
	cvt.u32.u16	%r57, %rs15;
	cvt.u32.u16	%r58, %rs31;
	mul.lo.s32 	%r59, %r58, %r57;
	shr.u32 	%r60, %r59, 16;
	cvt.u16.u32	%rs47, %r60;
	cvt.u32.u16	%r61, %rs16;
	cvt.u32.u16	%r62, %rs32;
	mul.lo.s32 	%r63, %r62, %r61;
	shr.u32 	%r64, %r63, 16;
	cvt.u16.u32	%rs48, %r64;
	st.param.v4.b16	[func_retval0+0], {%rs33, %rs34, %rs35, %rs36};
	st.param.v4.b16	[func_retval0+8], {%rs37, %rs38, %rs39, %rs40};
	st.param.v4.b16	[func_retval0+16], {%rs41, %rs42, %rs43, %rs44};
	st.param.v4.b16	[func_retval0+24], {%rs45, %rs46, %rs47, %rs48};
	ret;
}

	.weak	_Z6mul_hiii
.func  (.param .b32 func_retval0) _Z6mul_hiii(
	.param .b32 _Z6mul_hiii_param_0,
	.param .b32 _Z6mul_hiii_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .s64 	%rl<5>;

	ld.param.u32 	%r1, [_Z6mul_hiii_param_0];
	ld.param.u32 	%r2, [_Z6mul_hiii_param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r2;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r3, %rl4;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

	.weak	_Z6mul_hiDv2_iS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv2_iS_(
	.param .align 8 .b8 _Z6mul_hiDv2_iS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv2_iS__param_1[8]
)
{
	.reg .s32 	%r<7>;
	.reg .s64 	%rl<9>;

	ld.param.v2.u32 	{%r1, %r2}, [_Z6mul_hiDv2_iS__param_0];
	ld.param.v2.u32 	{%r3, %r4}, [_Z6mul_hiDv2_iS__param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r3;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r5, %rl4;
	cvt.s64.s32	%rl5, %r2;
	cvt.s64.s32	%rl6, %r4;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r6, %rl8;
	st.param.v2.b32	[func_retval0+0], {%r5, %r6};
	ret;
}

	.weak	_Z6mul_hiDv3_iS_
.func  (.param .align 16 .b8 func_retval0[12]) _Z6mul_hiDv3_iS_(
	.param .align 16 .b8 _Z6mul_hiDv3_iS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv3_iS__param_1[16]
)
{
	.reg .s32 	%r<13>;
	.reg .s64 	%rl<13>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r7}, [_Z6mul_hiDv3_iS__param_0];
	ld.param.v4.u32 	{%r4, %r5, %r6, %r8}, [_Z6mul_hiDv3_iS__param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r4;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r9, %rl4;
	cvt.s64.s32	%rl5, %r2;
	cvt.s64.s32	%rl6, %r5;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r10, %rl8;
	cvt.s64.s32	%rl9, %r3;
	cvt.s64.s32	%rl10, %r6;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r11, %rl12;
	st.param.v4.b32	[func_retval0+0], {%r9, %r10, %r11, %r12};
	ret;
}

	.weak	_Z6mul_hiDv4_iS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv4_iS_(
	.param .align 16 .b8 _Z6mul_hiDv4_iS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv4_iS__param_1[16]
)
{
	.reg .s32 	%r<13>;
	.reg .s64 	%rl<17>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv4_iS__param_0];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv4_iS__param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r5;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r9, %rl4;
	cvt.s64.s32	%rl5, %r2;
	cvt.s64.s32	%rl6, %r6;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r10, %rl8;
	cvt.s64.s32	%rl9, %r3;
	cvt.s64.s32	%rl10, %r7;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r11, %rl12;
	cvt.s64.s32	%rl13, %r4;
	cvt.s64.s32	%rl14, %r8;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r12, %rl16;
	st.param.v4.b32	[func_retval0+0], {%r9, %r10, %r11, %r12};
	ret;
}

	.weak	_Z6mul_hiDv8_iS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv8_iS_(
	.param .align 32 .b8 _Z6mul_hiDv8_iS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv8_iS__param_1[32]
)
{
	.reg .s32 	%r<25>;
	.reg .s64 	%rl<33>;

	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv8_iS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv8_iS__param_0];
	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z6mul_hiDv8_iS__param_1+16];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z6mul_hiDv8_iS__param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r9;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r17, %rl4;
	cvt.s64.s32	%rl5, %r2;
	cvt.s64.s32	%rl6, %r10;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r18, %rl8;
	cvt.s64.s32	%rl9, %r3;
	cvt.s64.s32	%rl10, %r11;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r19, %rl12;
	cvt.s64.s32	%rl13, %r4;
	cvt.s64.s32	%rl14, %r12;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r20, %rl16;
	cvt.s64.s32	%rl17, %r5;
	cvt.s64.s32	%rl18, %r13;
	mul.lo.s64 	%rl19, %rl18, %rl17;
	shr.u64 	%rl20, %rl19, 32;
	cvt.u32.u64	%r21, %rl20;
	cvt.s64.s32	%rl21, %r6;
	cvt.s64.s32	%rl22, %r14;
	mul.lo.s64 	%rl23, %rl22, %rl21;
	shr.u64 	%rl24, %rl23, 32;
	cvt.u32.u64	%r22, %rl24;
	cvt.s64.s32	%rl25, %r7;
	cvt.s64.s32	%rl26, %r15;
	mul.lo.s64 	%rl27, %rl26, %rl25;
	shr.u64 	%rl28, %rl27, 32;
	cvt.u32.u64	%r23, %rl28;
	cvt.s64.s32	%rl29, %r8;
	cvt.s64.s32	%rl30, %r16;
	mul.lo.s64 	%rl31, %rl30, %rl29;
	shr.u64 	%rl32, %rl31, 32;
	cvt.u32.u64	%r24, %rl32;
	st.param.v4.b32	[func_retval0+0], {%r17, %r18, %r19, %r20};
	st.param.v4.b32	[func_retval0+16], {%r21, %r22, %r23, %r24};
	ret;
}

	.weak	_Z6mul_hiDv16_iS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z6mul_hiDv16_iS_(
	.param .align 64 .b8 _Z6mul_hiDv16_iS__param_0[64],
	.param .align 64 .b8 _Z6mul_hiDv16_iS__param_1[64]
)
{
	.reg .s32 	%r<49>;
	.reg .s64 	%rl<65>;

	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z6mul_hiDv16_iS__param_0+48];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z6mul_hiDv16_iS__param_0+32];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv16_iS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv16_iS__param_0];
	ld.param.v4.u32 	{%r29, %r30, %r31, %r32}, [_Z6mul_hiDv16_iS__param_1+48];
	ld.param.v4.u32 	{%r25, %r26, %r27, %r28}, [_Z6mul_hiDv16_iS__param_1+32];
	ld.param.v4.u32 	{%r21, %r22, %r23, %r24}, [_Z6mul_hiDv16_iS__param_1+16];
	ld.param.v4.u32 	{%r17, %r18, %r19, %r20}, [_Z6mul_hiDv16_iS__param_1];
	cvt.s64.s32	%rl1, %r1;
	cvt.s64.s32	%rl2, %r17;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r33, %rl4;
	cvt.s64.s32	%rl5, %r2;
	cvt.s64.s32	%rl6, %r18;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r34, %rl8;
	cvt.s64.s32	%rl9, %r3;
	cvt.s64.s32	%rl10, %r19;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r35, %rl12;
	cvt.s64.s32	%rl13, %r4;
	cvt.s64.s32	%rl14, %r20;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r36, %rl16;
	cvt.s64.s32	%rl17, %r5;
	cvt.s64.s32	%rl18, %r21;
	mul.lo.s64 	%rl19, %rl18, %rl17;
	shr.u64 	%rl20, %rl19, 32;
	cvt.u32.u64	%r37, %rl20;
	cvt.s64.s32	%rl21, %r6;
	cvt.s64.s32	%rl22, %r22;
	mul.lo.s64 	%rl23, %rl22, %rl21;
	shr.u64 	%rl24, %rl23, 32;
	cvt.u32.u64	%r38, %rl24;
	cvt.s64.s32	%rl25, %r7;
	cvt.s64.s32	%rl26, %r23;
	mul.lo.s64 	%rl27, %rl26, %rl25;
	shr.u64 	%rl28, %rl27, 32;
	cvt.u32.u64	%r39, %rl28;
	cvt.s64.s32	%rl29, %r8;
	cvt.s64.s32	%rl30, %r24;
	mul.lo.s64 	%rl31, %rl30, %rl29;
	shr.u64 	%rl32, %rl31, 32;
	cvt.u32.u64	%r40, %rl32;
	cvt.s64.s32	%rl33, %r9;
	cvt.s64.s32	%rl34, %r25;
	mul.lo.s64 	%rl35, %rl34, %rl33;
	shr.u64 	%rl36, %rl35, 32;
	cvt.u32.u64	%r41, %rl36;
	cvt.s64.s32	%rl37, %r10;
	cvt.s64.s32	%rl38, %r26;
	mul.lo.s64 	%rl39, %rl38, %rl37;
	shr.u64 	%rl40, %rl39, 32;
	cvt.u32.u64	%r42, %rl40;
	cvt.s64.s32	%rl41, %r11;
	cvt.s64.s32	%rl42, %r27;
	mul.lo.s64 	%rl43, %rl42, %rl41;
	shr.u64 	%rl44, %rl43, 32;
	cvt.u32.u64	%r43, %rl44;
	cvt.s64.s32	%rl45, %r12;
	cvt.s64.s32	%rl46, %r28;
	mul.lo.s64 	%rl47, %rl46, %rl45;
	shr.u64 	%rl48, %rl47, 32;
	cvt.u32.u64	%r44, %rl48;
	cvt.s64.s32	%rl49, %r13;
	cvt.s64.s32	%rl50, %r29;
	mul.lo.s64 	%rl51, %rl50, %rl49;
	shr.u64 	%rl52, %rl51, 32;
	cvt.u32.u64	%r45, %rl52;
	cvt.s64.s32	%rl53, %r14;
	cvt.s64.s32	%rl54, %r30;
	mul.lo.s64 	%rl55, %rl54, %rl53;
	shr.u64 	%rl56, %rl55, 32;
	cvt.u32.u64	%r46, %rl56;
	cvt.s64.s32	%rl57, %r15;
	cvt.s64.s32	%rl58, %r31;
	mul.lo.s64 	%rl59, %rl58, %rl57;
	shr.u64 	%rl60, %rl59, 32;
	cvt.u32.u64	%r47, %rl60;
	cvt.s64.s32	%rl61, %r16;
	cvt.s64.s32	%rl62, %r32;
	mul.lo.s64 	%rl63, %rl62, %rl61;
	shr.u64 	%rl64, %rl63, 32;
	cvt.u32.u64	%r48, %rl64;
	st.param.v4.b32	[func_retval0+0], {%r33, %r34, %r35, %r36};
	st.param.v4.b32	[func_retval0+16], {%r37, %r38, %r39, %r40};
	st.param.v4.b32	[func_retval0+32], {%r41, %r42, %r43, %r44};
	st.param.v4.b32	[func_retval0+48], {%r45, %r46, %r47, %r48};
	ret;
}

	.weak	_Z6mul_hijj
.func  (.param .b32 func_retval0) _Z6mul_hijj(
	.param .b32 _Z6mul_hijj_param_0,
	.param .b32 _Z6mul_hijj_param_1
)
{
	.reg .s32 	%r<4>;
	.reg .s64 	%rl<5>;

	ld.param.u32 	%r1, [_Z6mul_hijj_param_0];
	ld.param.u32 	%r2, [_Z6mul_hijj_param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r2;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r3, %rl4;
	st.param.b32	[func_retval0+0], %r3;
	ret;
}

	.weak	_Z6mul_hiDv2_jS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z6mul_hiDv2_jS_(
	.param .align 8 .b8 _Z6mul_hiDv2_jS__param_0[8],
	.param .align 8 .b8 _Z6mul_hiDv2_jS__param_1[8]
)
{
	.reg .s32 	%r<7>;
	.reg .s64 	%rl<9>;

	ld.param.v2.u32 	{%r1, %r2}, [_Z6mul_hiDv2_jS__param_0];
	ld.param.v2.u32 	{%r3, %r4}, [_Z6mul_hiDv2_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r3;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r5, %rl4;
	cvt.u64.u32	%rl5, %r2;
	cvt.u64.u32	%rl6, %r4;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r6, %rl8;
	st.param.v2.b32	[func_retval0+0], {%r5, %r6};
	ret;
}

	.weak	_Z6mul_hiDv3_jS_
.func  (.param .align 16 .b8 func_retval0[12]) _Z6mul_hiDv3_jS_(
	.param .align 16 .b8 _Z6mul_hiDv3_jS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv3_jS__param_1[16]
)
{
	.reg .s32 	%r<13>;
	.reg .s64 	%rl<13>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r7}, [_Z6mul_hiDv3_jS__param_0];
	ld.param.v4.u32 	{%r4, %r5, %r6, %r8}, [_Z6mul_hiDv3_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r4;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r9, %rl4;
	cvt.u64.u32	%rl5, %r2;
	cvt.u64.u32	%rl6, %r5;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r10, %rl8;
	cvt.u64.u32	%rl9, %r3;
	cvt.u64.u32	%rl10, %r6;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r11, %rl12;
	st.param.v4.b32	[func_retval0+0], {%r9, %r10, %r11, %r12};
	ret;
}

	.weak	_Z6mul_hiDv4_jS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv4_jS_(
	.param .align 16 .b8 _Z6mul_hiDv4_jS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv4_jS__param_1[16]
)
{
	.reg .s32 	%r<13>;
	.reg .s64 	%rl<17>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv4_jS__param_0];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv4_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r5;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r9, %rl4;
	cvt.u64.u32	%rl5, %r2;
	cvt.u64.u32	%rl6, %r6;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r10, %rl8;
	cvt.u64.u32	%rl9, %r3;
	cvt.u64.u32	%rl10, %r7;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r11, %rl12;
	cvt.u64.u32	%rl13, %r4;
	cvt.u64.u32	%rl14, %r8;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r12, %rl16;
	st.param.v4.b32	[func_retval0+0], {%r9, %r10, %r11, %r12};
	ret;
}

	.weak	_Z6mul_hiDv8_jS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv8_jS_(
	.param .align 32 .b8 _Z6mul_hiDv8_jS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv8_jS__param_1[32]
)
{
	.reg .s32 	%r<25>;
	.reg .s64 	%rl<33>;

	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv8_jS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv8_jS__param_0];
	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z6mul_hiDv8_jS__param_1+16];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z6mul_hiDv8_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r9;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r17, %rl4;
	cvt.u64.u32	%rl5, %r2;
	cvt.u64.u32	%rl6, %r10;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r18, %rl8;
	cvt.u64.u32	%rl9, %r3;
	cvt.u64.u32	%rl10, %r11;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r19, %rl12;
	cvt.u64.u32	%rl13, %r4;
	cvt.u64.u32	%rl14, %r12;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r20, %rl16;
	cvt.u64.u32	%rl17, %r5;
	cvt.u64.u32	%rl18, %r13;
	mul.lo.s64 	%rl19, %rl18, %rl17;
	shr.u64 	%rl20, %rl19, 32;
	cvt.u32.u64	%r21, %rl20;
	cvt.u64.u32	%rl21, %r6;
	cvt.u64.u32	%rl22, %r14;
	mul.lo.s64 	%rl23, %rl22, %rl21;
	shr.u64 	%rl24, %rl23, 32;
	cvt.u32.u64	%r22, %rl24;
	cvt.u64.u32	%rl25, %r7;
	cvt.u64.u32	%rl26, %r15;
	mul.lo.s64 	%rl27, %rl26, %rl25;
	shr.u64 	%rl28, %rl27, 32;
	cvt.u32.u64	%r23, %rl28;
	cvt.u64.u32	%rl29, %r8;
	cvt.u64.u32	%rl30, %r16;
	mul.lo.s64 	%rl31, %rl30, %rl29;
	shr.u64 	%rl32, %rl31, 32;
	cvt.u32.u64	%r24, %rl32;
	st.param.v4.b32	[func_retval0+0], {%r17, %r18, %r19, %r20};
	st.param.v4.b32	[func_retval0+16], {%r21, %r22, %r23, %r24};
	ret;
}

	.weak	_Z6mul_hiDv16_jS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z6mul_hiDv16_jS_(
	.param .align 64 .b8 _Z6mul_hiDv16_jS__param_0[64],
	.param .align 64 .b8 _Z6mul_hiDv16_jS__param_1[64]
)
{
	.reg .s32 	%r<49>;
	.reg .s64 	%rl<65>;

	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z6mul_hiDv16_jS__param_0+48];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z6mul_hiDv16_jS__param_0+32];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z6mul_hiDv16_jS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z6mul_hiDv16_jS__param_0];
	ld.param.v4.u32 	{%r29, %r30, %r31, %r32}, [_Z6mul_hiDv16_jS__param_1+48];
	ld.param.v4.u32 	{%r25, %r26, %r27, %r28}, [_Z6mul_hiDv16_jS__param_1+32];
	ld.param.v4.u32 	{%r21, %r22, %r23, %r24}, [_Z6mul_hiDv16_jS__param_1+16];
	ld.param.v4.u32 	{%r17, %r18, %r19, %r20}, [_Z6mul_hiDv16_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	cvt.u64.u32	%rl2, %r17;
	mul.lo.s64 	%rl3, %rl2, %rl1;
	shr.u64 	%rl4, %rl3, 32;
	cvt.u32.u64	%r33, %rl4;
	cvt.u64.u32	%rl5, %r2;
	cvt.u64.u32	%rl6, %r18;
	mul.lo.s64 	%rl7, %rl6, %rl5;
	shr.u64 	%rl8, %rl7, 32;
	cvt.u32.u64	%r34, %rl8;
	cvt.u64.u32	%rl9, %r3;
	cvt.u64.u32	%rl10, %r19;
	mul.lo.s64 	%rl11, %rl10, %rl9;
	shr.u64 	%rl12, %rl11, 32;
	cvt.u32.u64	%r35, %rl12;
	cvt.u64.u32	%rl13, %r4;
	cvt.u64.u32	%rl14, %r20;
	mul.lo.s64 	%rl15, %rl14, %rl13;
	shr.u64 	%rl16, %rl15, 32;
	cvt.u32.u64	%r36, %rl16;
	cvt.u64.u32	%rl17, %r5;
	cvt.u64.u32	%rl18, %r21;
	mul.lo.s64 	%rl19, %rl18, %rl17;
	shr.u64 	%rl20, %rl19, 32;
	cvt.u32.u64	%r37, %rl20;
	cvt.u64.u32	%rl21, %r6;
	cvt.u64.u32	%rl22, %r22;
	mul.lo.s64 	%rl23, %rl22, %rl21;
	shr.u64 	%rl24, %rl23, 32;
	cvt.u32.u64	%r38, %rl24;
	cvt.u64.u32	%rl25, %r7;
	cvt.u64.u32	%rl26, %r23;
	mul.lo.s64 	%rl27, %rl26, %rl25;
	shr.u64 	%rl28, %rl27, 32;
	cvt.u32.u64	%r39, %rl28;
	cvt.u64.u32	%rl29, %r8;
	cvt.u64.u32	%rl30, %r24;
	mul.lo.s64 	%rl31, %rl30, %rl29;
	shr.u64 	%rl32, %rl31, 32;
	cvt.u32.u64	%r40, %rl32;
	cvt.u64.u32	%rl33, %r9;
	cvt.u64.u32	%rl34, %r25;
	mul.lo.s64 	%rl35, %rl34, %rl33;
	shr.u64 	%rl36, %rl35, 32;
	cvt.u32.u64	%r41, %rl36;
	cvt.u64.u32	%rl37, %r10;
	cvt.u64.u32	%rl38, %r26;
	mul.lo.s64 	%rl39, %rl38, %rl37;
	shr.u64 	%rl40, %rl39, 32;
	cvt.u32.u64	%r42, %rl40;
	cvt.u64.u32	%rl41, %r11;
	cvt.u64.u32	%rl42, %r27;
	mul.lo.s64 	%rl43, %rl42, %rl41;
	shr.u64 	%rl44, %rl43, 32;
	cvt.u32.u64	%r43, %rl44;
	cvt.u64.u32	%rl45, %r12;
	cvt.u64.u32	%rl46, %r28;
	mul.lo.s64 	%rl47, %rl46, %rl45;
	shr.u64 	%rl48, %rl47, 32;
	cvt.u32.u64	%r44, %rl48;
	cvt.u64.u32	%rl49, %r13;
	cvt.u64.u32	%rl50, %r29;
	mul.lo.s64 	%rl51, %rl50, %rl49;
	shr.u64 	%rl52, %rl51, 32;
	cvt.u32.u64	%r45, %rl52;
	cvt.u64.u32	%rl53, %r14;
	cvt.u64.u32	%rl54, %r30;
	mul.lo.s64 	%rl55, %rl54, %rl53;
	shr.u64 	%rl56, %rl55, 32;
	cvt.u32.u64	%r46, %rl56;
	cvt.u64.u32	%rl57, %r15;
	cvt.u64.u32	%rl58, %r31;
	mul.lo.s64 	%rl59, %rl58, %rl57;
	shr.u64 	%rl60, %rl59, 32;
	cvt.u32.u64	%r47, %rl60;
	cvt.u64.u32	%rl61, %r16;
	cvt.u64.u32	%rl62, %r32;
	mul.lo.s64 	%rl63, %rl62, %rl61;
	shr.u64 	%rl64, %rl63, 32;
	cvt.u32.u64	%r48, %rl64;
	st.param.v4.b32	[func_retval0+0], {%r33, %r34, %r35, %r36};
	st.param.v4.b32	[func_retval0+16], {%r37, %r38, %r39, %r40};
	st.param.v4.b32	[func_retval0+32], {%r41, %r42, %r43, %r44};
	st.param.v4.b32	[func_retval0+48], {%r45, %r46, %r47, %r48};
	ret;
}

	.weak	_Z6mul_hiDv2_lS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv2_lS_(
	.param .align 16 .b8 _Z6mul_hiDv2_lS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv2_lS__param_1[16]
)
{
	.reg .s64 	%rl<37>;

	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv2_lS__param_0];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv2_lS__param_1];
	shr.s64 	%rl5, %rl1, 32;
	and.b64  	%rl6, %rl1, 4294967295;
	shr.s64 	%rl7, %rl3, 32;
	and.b64  	%rl8, %rl3, 4294967295;
	mul.lo.s64 	%rl9, %rl8, %rl5;
	mul.lo.s64 	%rl10, %rl8, %rl6;
	shr.u64 	%rl11, %rl10, 32;
	mad.lo.s64 	%rl12, %rl7, %rl6, %rl11;
	shr.s64 	%rl13, %rl9, 1;
	shr.s64 	%rl14, %rl12, 1;
	add.s64 	%rl15, %rl14, %rl13;
	and.b64  	%rl16, %rl9, %rl12;
	and.b64  	%rl17, %rl16, 1;
	add.s64 	%rl18, %rl15, %rl17;
	shr.s64 	%rl19, %rl18, 31;
	mad.lo.s64 	%rl20, %rl7, %rl5, %rl19;
	shr.s64 	%rl21, %rl2, 32;
	and.b64  	%rl22, %rl2, 4294967295;
	shr.s64 	%rl23, %rl4, 32;
	and.b64  	%rl24, %rl4, 4294967295;
	mul.lo.s64 	%rl25, %rl24, %rl21;
	mul.lo.s64 	%rl26, %rl24, %rl22;
	shr.u64 	%rl27, %rl26, 32;
	mad.lo.s64 	%rl28, %rl23, %rl22, %rl27;
	shr.s64 	%rl29, %rl25, 1;
	shr.s64 	%rl30, %rl28, 1;
	add.s64 	%rl31, %rl30, %rl29;
	and.b64  	%rl32, %rl25, %rl28;
	and.b64  	%rl33, %rl32, 1;
	add.s64 	%rl34, %rl31, %rl33;
	shr.s64 	%rl35, %rl34, 31;
	mad.lo.s64 	%rl36, %rl23, %rl21, %rl35;
	st.param.v2.b64	[func_retval0+0], {%rl20, %rl36};
	ret;
}

	.weak	_Z6mul_hiDv3_lS_
.func  (.param .align 32 .b8 func_retval0[24]) _Z6mul_hiDv3_lS_(
	.param .align 32 .b8 _Z6mul_hiDv3_lS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv3_lS__param_1[32]
)
{
	.reg .s64 	%rl<58>;

	ld.param.v2.u64 	{%rl3, %rl7}, [_Z6mul_hiDv3_lS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv3_lS__param_0];
	ld.param.v2.u64 	{%rl6, %rl8}, [_Z6mul_hiDv3_lS__param_1+16];
	ld.param.v2.u64 	{%rl4, %rl5}, [_Z6mul_hiDv3_lS__param_1];
	shr.s64 	%rl9, %rl1, 32;
	and.b64  	%rl10, %rl1, 4294967295;
	shr.s64 	%rl11, %rl4, 32;
	and.b64  	%rl12, %rl4, 4294967295;
	mul.lo.s64 	%rl13, %rl12, %rl9;
	mul.lo.s64 	%rl14, %rl12, %rl10;
	shr.u64 	%rl15, %rl14, 32;
	mad.lo.s64 	%rl16, %rl11, %rl10, %rl15;
	shr.s64 	%rl17, %rl13, 1;
	shr.s64 	%rl18, %rl16, 1;
	add.s64 	%rl19, %rl18, %rl17;
	and.b64  	%rl20, %rl13, %rl16;
	and.b64  	%rl21, %rl20, 1;
	add.s64 	%rl22, %rl19, %rl21;
	shr.s64 	%rl23, %rl22, 31;
	mad.lo.s64 	%rl24, %rl11, %rl9, %rl23;
	shr.s64 	%rl25, %rl2, 32;
	and.b64  	%rl26, %rl2, 4294967295;
	shr.s64 	%rl27, %rl5, 32;
	and.b64  	%rl28, %rl5, 4294967295;
	mul.lo.s64 	%rl29, %rl28, %rl25;
	mul.lo.s64 	%rl30, %rl28, %rl26;
	shr.u64 	%rl31, %rl30, 32;
	mad.lo.s64 	%rl32, %rl27, %rl26, %rl31;
	shr.s64 	%rl33, %rl29, 1;
	shr.s64 	%rl34, %rl32, 1;
	add.s64 	%rl35, %rl34, %rl33;
	and.b64  	%rl36, %rl29, %rl32;
	and.b64  	%rl37, %rl36, 1;
	add.s64 	%rl38, %rl35, %rl37;
	shr.s64 	%rl39, %rl38, 31;
	mad.lo.s64 	%rl40, %rl27, %rl25, %rl39;
	shr.s64 	%rl41, %rl3, 32;
	and.b64  	%rl42, %rl3, 4294967295;
	shr.s64 	%rl43, %rl6, 32;
	and.b64  	%rl44, %rl6, 4294967295;
	mul.lo.s64 	%rl45, %rl44, %rl41;
	mul.lo.s64 	%rl46, %rl44, %rl42;
	shr.u64 	%rl47, %rl46, 32;
	mad.lo.s64 	%rl48, %rl43, %rl42, %rl47;
	shr.s64 	%rl49, %rl45, 1;
	shr.s64 	%rl50, %rl48, 1;
	add.s64 	%rl51, %rl50, %rl49;
	and.b64  	%rl52, %rl45, %rl48;
	and.b64  	%rl53, %rl52, 1;
	add.s64 	%rl54, %rl51, %rl53;
	shr.s64 	%rl55, %rl54, 31;
	mad.lo.s64 	%rl56, %rl43, %rl41, %rl55;
	st.param.v2.b64	[func_retval0+0], {%rl24, %rl40};
	st.param.v2.b64	[func_retval0+16], {%rl56, %rl57};
	ret;
}

	.weak	_Z6mul_hiDv4_lS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv4_lS_(
	.param .align 32 .b8 _Z6mul_hiDv4_lS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv4_lS__param_1[32]
)
{
	.reg .s64 	%rl<73>;

	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv4_lS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv4_lS__param_0];
	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv4_lS__param_1+16];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv4_lS__param_1];
	shr.s64 	%rl9, %rl1, 32;
	and.b64  	%rl10, %rl1, 4294967295;
	shr.s64 	%rl11, %rl5, 32;
	and.b64  	%rl12, %rl5, 4294967295;
	mul.lo.s64 	%rl13, %rl12, %rl9;
	mul.lo.s64 	%rl14, %rl12, %rl10;
	shr.u64 	%rl15, %rl14, 32;
	mad.lo.s64 	%rl16, %rl11, %rl10, %rl15;
	shr.s64 	%rl17, %rl13, 1;
	shr.s64 	%rl18, %rl16, 1;
	add.s64 	%rl19, %rl18, %rl17;
	and.b64  	%rl20, %rl13, %rl16;
	and.b64  	%rl21, %rl20, 1;
	add.s64 	%rl22, %rl19, %rl21;
	shr.s64 	%rl23, %rl22, 31;
	mad.lo.s64 	%rl24, %rl11, %rl9, %rl23;
	shr.s64 	%rl25, %rl2, 32;
	and.b64  	%rl26, %rl2, 4294967295;
	shr.s64 	%rl27, %rl6, 32;
	and.b64  	%rl28, %rl6, 4294967295;
	mul.lo.s64 	%rl29, %rl28, %rl25;
	mul.lo.s64 	%rl30, %rl28, %rl26;
	shr.u64 	%rl31, %rl30, 32;
	mad.lo.s64 	%rl32, %rl27, %rl26, %rl31;
	shr.s64 	%rl33, %rl29, 1;
	shr.s64 	%rl34, %rl32, 1;
	add.s64 	%rl35, %rl34, %rl33;
	and.b64  	%rl36, %rl29, %rl32;
	and.b64  	%rl37, %rl36, 1;
	add.s64 	%rl38, %rl35, %rl37;
	shr.s64 	%rl39, %rl38, 31;
	mad.lo.s64 	%rl40, %rl27, %rl25, %rl39;
	shr.s64 	%rl41, %rl3, 32;
	and.b64  	%rl42, %rl3, 4294967295;
	shr.s64 	%rl43, %rl7, 32;
	and.b64  	%rl44, %rl7, 4294967295;
	mul.lo.s64 	%rl45, %rl44, %rl41;
	mul.lo.s64 	%rl46, %rl44, %rl42;
	shr.u64 	%rl47, %rl46, 32;
	mad.lo.s64 	%rl48, %rl43, %rl42, %rl47;
	shr.s64 	%rl49, %rl45, 1;
	shr.s64 	%rl50, %rl48, 1;
	add.s64 	%rl51, %rl50, %rl49;
	and.b64  	%rl52, %rl45, %rl48;
	and.b64  	%rl53, %rl52, 1;
	add.s64 	%rl54, %rl51, %rl53;
	shr.s64 	%rl55, %rl54, 31;
	mad.lo.s64 	%rl56, %rl43, %rl41, %rl55;
	shr.s64 	%rl57, %rl4, 32;
	and.b64  	%rl58, %rl4, 4294967295;
	shr.s64 	%rl59, %rl8, 32;
	and.b64  	%rl60, %rl8, 4294967295;
	mul.lo.s64 	%rl61, %rl60, %rl57;
	mul.lo.s64 	%rl62, %rl60, %rl58;
	shr.u64 	%rl63, %rl62, 32;
	mad.lo.s64 	%rl64, %rl59, %rl58, %rl63;
	shr.s64 	%rl65, %rl61, 1;
	shr.s64 	%rl66, %rl64, 1;
	add.s64 	%rl67, %rl66, %rl65;
	and.b64  	%rl68, %rl61, %rl64;
	and.b64  	%rl69, %rl68, 1;
	add.s64 	%rl70, %rl67, %rl69;
	shr.s64 	%rl71, %rl70, 31;
	mad.lo.s64 	%rl72, %rl59, %rl57, %rl71;
	st.param.v2.b64	[func_retval0+0], {%rl24, %rl40};
	st.param.v2.b64	[func_retval0+16], {%rl56, %rl72};
	ret;
}

	.weak	_Z6mul_hiDv8_lS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z6mul_hiDv8_lS_(
	.param .align 64 .b8 _Z6mul_hiDv8_lS__param_0[64],
	.param .align 64 .b8 _Z6mul_hiDv8_lS__param_1[64]
)
{
	.reg .s64 	%rl<145>;

	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv8_lS__param_0+48];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv8_lS__param_0+32];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv8_lS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv8_lS__param_0];
	ld.param.v2.u64 	{%rl15, %rl16}, [_Z6mul_hiDv8_lS__param_1+48];
	ld.param.v2.u64 	{%rl13, %rl14}, [_Z6mul_hiDv8_lS__param_1+32];
	ld.param.v2.u64 	{%rl11, %rl12}, [_Z6mul_hiDv8_lS__param_1+16];
	ld.param.v2.u64 	{%rl9, %rl10}, [_Z6mul_hiDv8_lS__param_1];
	shr.s64 	%rl17, %rl1, 32;
	and.b64  	%rl18, %rl1, 4294967295;
	shr.s64 	%rl19, %rl9, 32;
	and.b64  	%rl20, %rl9, 4294967295;
	mul.lo.s64 	%rl21, %rl20, %rl17;
	mul.lo.s64 	%rl22, %rl20, %rl18;
	shr.u64 	%rl23, %rl22, 32;
	mad.lo.s64 	%rl24, %rl19, %rl18, %rl23;
	shr.s64 	%rl25, %rl21, 1;
	shr.s64 	%rl26, %rl24, 1;
	add.s64 	%rl27, %rl26, %rl25;
	and.b64  	%rl28, %rl21, %rl24;
	and.b64  	%rl29, %rl28, 1;
	add.s64 	%rl30, %rl27, %rl29;
	shr.s64 	%rl31, %rl30, 31;
	mad.lo.s64 	%rl32, %rl19, %rl17, %rl31;
	shr.s64 	%rl33, %rl2, 32;
	and.b64  	%rl34, %rl2, 4294967295;
	shr.s64 	%rl35, %rl10, 32;
	and.b64  	%rl36, %rl10, 4294967295;
	mul.lo.s64 	%rl37, %rl36, %rl33;
	mul.lo.s64 	%rl38, %rl36, %rl34;
	shr.u64 	%rl39, %rl38, 32;
	mad.lo.s64 	%rl40, %rl35, %rl34, %rl39;
	shr.s64 	%rl41, %rl37, 1;
	shr.s64 	%rl42, %rl40, 1;
	add.s64 	%rl43, %rl42, %rl41;
	and.b64  	%rl44, %rl37, %rl40;
	and.b64  	%rl45, %rl44, 1;
	add.s64 	%rl46, %rl43, %rl45;
	shr.s64 	%rl47, %rl46, 31;
	mad.lo.s64 	%rl48, %rl35, %rl33, %rl47;
	shr.s64 	%rl49, %rl3, 32;
	and.b64  	%rl50, %rl3, 4294967295;
	shr.s64 	%rl51, %rl11, 32;
	and.b64  	%rl52, %rl11, 4294967295;
	mul.lo.s64 	%rl53, %rl52, %rl49;
	mul.lo.s64 	%rl54, %rl52, %rl50;
	shr.u64 	%rl55, %rl54, 32;
	mad.lo.s64 	%rl56, %rl51, %rl50, %rl55;
	shr.s64 	%rl57, %rl53, 1;
	shr.s64 	%rl58, %rl56, 1;
	add.s64 	%rl59, %rl58, %rl57;
	and.b64  	%rl60, %rl53, %rl56;
	and.b64  	%rl61, %rl60, 1;
	add.s64 	%rl62, %rl59, %rl61;
	shr.s64 	%rl63, %rl62, 31;
	mad.lo.s64 	%rl64, %rl51, %rl49, %rl63;
	shr.s64 	%rl65, %rl4, 32;
	and.b64  	%rl66, %rl4, 4294967295;
	shr.s64 	%rl67, %rl12, 32;
	and.b64  	%rl68, %rl12, 4294967295;
	mul.lo.s64 	%rl69, %rl68, %rl65;
	mul.lo.s64 	%rl70, %rl68, %rl66;
	shr.u64 	%rl71, %rl70, 32;
	mad.lo.s64 	%rl72, %rl67, %rl66, %rl71;
	shr.s64 	%rl73, %rl69, 1;
	shr.s64 	%rl74, %rl72, 1;
	add.s64 	%rl75, %rl74, %rl73;
	and.b64  	%rl76, %rl69, %rl72;
	and.b64  	%rl77, %rl76, 1;
	add.s64 	%rl78, %rl75, %rl77;
	shr.s64 	%rl79, %rl78, 31;
	mad.lo.s64 	%rl80, %rl67, %rl65, %rl79;
	shr.s64 	%rl81, %rl5, 32;
	and.b64  	%rl82, %rl5, 4294967295;
	shr.s64 	%rl83, %rl13, 32;
	and.b64  	%rl84, %rl13, 4294967295;
	mul.lo.s64 	%rl85, %rl84, %rl81;
	mul.lo.s64 	%rl86, %rl84, %rl82;
	shr.u64 	%rl87, %rl86, 32;
	mad.lo.s64 	%rl88, %rl83, %rl82, %rl87;
	shr.s64 	%rl89, %rl85, 1;
	shr.s64 	%rl90, %rl88, 1;
	add.s64 	%rl91, %rl90, %rl89;
	and.b64  	%rl92, %rl85, %rl88;
	and.b64  	%rl93, %rl92, 1;
	add.s64 	%rl94, %rl91, %rl93;
	shr.s64 	%rl95, %rl94, 31;
	mad.lo.s64 	%rl96, %rl83, %rl81, %rl95;
	shr.s64 	%rl97, %rl6, 32;
	and.b64  	%rl98, %rl6, 4294967295;
	shr.s64 	%rl99, %rl14, 32;
	and.b64  	%rl100, %rl14, 4294967295;
	mul.lo.s64 	%rl101, %rl100, %rl97;
	mul.lo.s64 	%rl102, %rl100, %rl98;
	shr.u64 	%rl103, %rl102, 32;
	mad.lo.s64 	%rl104, %rl99, %rl98, %rl103;
	shr.s64 	%rl105, %rl101, 1;
	shr.s64 	%rl106, %rl104, 1;
	add.s64 	%rl107, %rl106, %rl105;
	and.b64  	%rl108, %rl101, %rl104;
	and.b64  	%rl109, %rl108, 1;
	add.s64 	%rl110, %rl107, %rl109;
	shr.s64 	%rl111, %rl110, 31;
	mad.lo.s64 	%rl112, %rl99, %rl97, %rl111;
	shr.s64 	%rl113, %rl7, 32;
	and.b64  	%rl114, %rl7, 4294967295;
	shr.s64 	%rl115, %rl15, 32;
	and.b64  	%rl116, %rl15, 4294967295;
	mul.lo.s64 	%rl117, %rl116, %rl113;
	mul.lo.s64 	%rl118, %rl116, %rl114;
	shr.u64 	%rl119, %rl118, 32;
	mad.lo.s64 	%rl120, %rl115, %rl114, %rl119;
	shr.s64 	%rl121, %rl117, 1;
	shr.s64 	%rl122, %rl120, 1;
	add.s64 	%rl123, %rl122, %rl121;
	and.b64  	%rl124, %rl117, %rl120;
	and.b64  	%rl125, %rl124, 1;
	add.s64 	%rl126, %rl123, %rl125;
	shr.s64 	%rl127, %rl126, 31;
	mad.lo.s64 	%rl128, %rl115, %rl113, %rl127;
	shr.s64 	%rl129, %rl8, 32;
	and.b64  	%rl130, %rl8, 4294967295;
	shr.s64 	%rl131, %rl16, 32;
	and.b64  	%rl132, %rl16, 4294967295;
	mul.lo.s64 	%rl133, %rl132, %rl129;
	mul.lo.s64 	%rl134, %rl132, %rl130;
	shr.u64 	%rl135, %rl134, 32;
	mad.lo.s64 	%rl136, %rl131, %rl130, %rl135;
	shr.s64 	%rl137, %rl133, 1;
	shr.s64 	%rl138, %rl136, 1;
	add.s64 	%rl139, %rl138, %rl137;
	and.b64  	%rl140, %rl133, %rl136;
	and.b64  	%rl141, %rl140, 1;
	add.s64 	%rl142, %rl139, %rl141;
	shr.s64 	%rl143, %rl142, 31;
	mad.lo.s64 	%rl144, %rl131, %rl129, %rl143;
	st.param.v2.b64	[func_retval0+0], {%rl32, %rl48};
	st.param.v2.b64	[func_retval0+16], {%rl64, %rl80};
	st.param.v2.b64	[func_retval0+32], {%rl96, %rl112};
	st.param.v2.b64	[func_retval0+48], {%rl128, %rl144};
	ret;
}

	.weak	_Z6mul_hiDv16_lS_
.func  (.param .align 128 .b8 func_retval0[128]) _Z6mul_hiDv16_lS_(
	.param .align 128 .b8 _Z6mul_hiDv16_lS__param_0[128],
	.param .align 128 .b8 _Z6mul_hiDv16_lS__param_1[128]
)
{
	.reg .s64 	%rl<49>;

	ld.param.v2.u64 	{%rl15, %rl16}, [_Z6mul_hiDv16_lS__param_0+112];
	ld.param.v2.u64 	{%rl13, %rl14}, [_Z6mul_hiDv16_lS__param_0+96];
	ld.param.v2.u64 	{%rl11, %rl12}, [_Z6mul_hiDv16_lS__param_0+80];
	ld.param.v2.u64 	{%rl9, %rl10}, [_Z6mul_hiDv16_lS__param_0+64];
	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv16_lS__param_0+48];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv16_lS__param_0+32];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv16_lS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv16_lS__param_0];
	ld.param.v2.u64 	{%rl31, %rl32}, [_Z6mul_hiDv16_lS__param_1+112];
	ld.param.v2.u64 	{%rl29, %rl30}, [_Z6mul_hiDv16_lS__param_1+96];
	ld.param.v2.u64 	{%rl27, %rl28}, [_Z6mul_hiDv16_lS__param_1+80];
	ld.param.v2.u64 	{%rl25, %rl26}, [_Z6mul_hiDv16_lS__param_1+64];
	ld.param.v2.u64 	{%rl23, %rl24}, [_Z6mul_hiDv16_lS__param_1+48];
	ld.param.v2.u64 	{%rl21, %rl22}, [_Z6mul_hiDv16_lS__param_1+32];
	ld.param.v2.u64 	{%rl19, %rl20}, [_Z6mul_hiDv16_lS__param_1+16];
	ld.param.v2.u64 	{%rl17, %rl18}, [_Z6mul_hiDv16_lS__param_1];
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .align 64 .b8 param0[64];
	st.param.v2.b64	[param0+0], {%rl1, %rl2};
	st.param.v2.b64	[param0+16], {%rl3, %rl4};
	st.param.v2.b64	[param0+32], {%rl5, %rl6};
	st.param.v2.b64	[param0+48], {%rl7, %rl8};
	.param .align 64 .b8 param1[64];
	st.param.v2.b64	[param1+0], {%rl17, %rl18};
	st.param.v2.b64	[param1+16], {%rl19, %rl20};
	st.param.v2.b64	[param1+32], {%rl21, %rl22};
	st.param.v2.b64	[param1+48], {%rl23, %rl24};
	.param .align 64 .b8 retval0[64];
	call.uni (retval0), 
	_Z6mul_hiDv8_lS_, 
	(
	param0, 
	param1
	);
	ld.param.v2.b64	{%rl33, %rl34}, [retval0+0];
	ld.param.v2.b64	{%rl35, %rl36}, [retval0+16];
	ld.param.v2.b64	{%rl37, %rl38}, [retval0+32];
	ld.param.v2.b64	{%rl39, %rl40}, [retval0+48];
	
	//{
	}// Callseq End 0
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .align 64 .b8 param0[64];
	st.param.v2.b64	[param0+0], {%rl9, %rl10};
	st.param.v2.b64	[param0+16], {%rl11, %rl12};
	st.param.v2.b64	[param0+32], {%rl13, %rl14};
	st.param.v2.b64	[param0+48], {%rl15, %rl16};
	.param .align 64 .b8 param1[64];
	st.param.v2.b64	[param1+0], {%rl25, %rl26};
	st.param.v2.b64	[param1+16], {%rl27, %rl28};
	st.param.v2.b64	[param1+32], {%rl29, %rl30};
	st.param.v2.b64	[param1+48], {%rl31, %rl32};
	.param .align 64 .b8 retval0[64];
	call.uni (retval0), 
	_Z6mul_hiDv8_lS_, 
	(
	param0, 
	param1
	);
	ld.param.v2.b64	{%rl41, %rl42}, [retval0+0];
	ld.param.v2.b64	{%rl43, %rl44}, [retval0+16];
	ld.param.v2.b64	{%rl45, %rl46}, [retval0+32];
	ld.param.v2.b64	{%rl47, %rl48}, [retval0+48];
	
	//{
	}// Callseq End 1
	st.param.v2.b64	[func_retval0+0], {%rl33, %rl34};
	st.param.v2.b64	[func_retval0+16], {%rl35, %rl36};
	st.param.v2.b64	[func_retval0+32], {%rl37, %rl38};
	st.param.v2.b64	[func_retval0+48], {%rl39, %rl40};
	st.param.v2.b64	[func_retval0+64], {%rl41, %rl42};
	st.param.v2.b64	[func_retval0+80], {%rl43, %rl44};
	st.param.v2.b64	[func_retval0+96], {%rl45, %rl46};
	st.param.v2.b64	[func_retval0+112], {%rl47, %rl48};
	ret;
}

	.weak	_Z6mul_hiDv2_mS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z6mul_hiDv2_mS_(
	.param .align 16 .b8 _Z6mul_hiDv2_mS__param_0[16],
	.param .align 16 .b8 _Z6mul_hiDv2_mS__param_1[16]
)
{
	.reg .s64 	%rl<37>;

	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv2_mS__param_0];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv2_mS__param_1];
	shr.u64 	%rl5, %rl1, 32;
	and.b64  	%rl6, %rl1, 4294967295;
	shr.u64 	%rl7, %rl3, 32;
	and.b64  	%rl8, %rl3, 4294967295;
	mul.lo.s64 	%rl9, %rl8, %rl5;
	mul.lo.s64 	%rl10, %rl8, %rl6;
	shr.u64 	%rl11, %rl10, 32;
	mad.lo.s64 	%rl12, %rl7, %rl6, %rl11;
	shr.u64 	%rl13, %rl9, 1;
	shr.u64 	%rl14, %rl12, 1;
	add.s64 	%rl15, %rl14, %rl13;
	and.b64  	%rl16, %rl9, %rl12;
	and.b64  	%rl17, %rl16, 1;
	add.s64 	%rl18, %rl15, %rl17;
	shr.u64 	%rl19, %rl18, 31;
	mad.lo.s64 	%rl20, %rl7, %rl5, %rl19;
	shr.u64 	%rl21, %rl2, 32;
	and.b64  	%rl22, %rl2, 4294967295;
	shr.u64 	%rl23, %rl4, 32;
	and.b64  	%rl24, %rl4, 4294967295;
	mul.lo.s64 	%rl25, %rl24, %rl21;
	mul.lo.s64 	%rl26, %rl24, %rl22;
	shr.u64 	%rl27, %rl26, 32;
	mad.lo.s64 	%rl28, %rl23, %rl22, %rl27;
	shr.u64 	%rl29, %rl25, 1;
	shr.u64 	%rl30, %rl28, 1;
	add.s64 	%rl31, %rl30, %rl29;
	and.b64  	%rl32, %rl25, %rl28;
	and.b64  	%rl33, %rl32, 1;
	add.s64 	%rl34, %rl31, %rl33;
	shr.u64 	%rl35, %rl34, 31;
	mad.lo.s64 	%rl36, %rl23, %rl21, %rl35;
	st.param.v2.b64	[func_retval0+0], {%rl20, %rl36};
	ret;
}

	.weak	_Z6mul_hiDv3_mS_
.func  (.param .align 32 .b8 func_retval0[24]) _Z6mul_hiDv3_mS_(
	.param .align 32 .b8 _Z6mul_hiDv3_mS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv3_mS__param_1[32]
)
{
	.reg .s64 	%rl<58>;

	ld.param.v2.u64 	{%rl3, %rl7}, [_Z6mul_hiDv3_mS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv3_mS__param_0];
	ld.param.v2.u64 	{%rl6, %rl8}, [_Z6mul_hiDv3_mS__param_1+16];
	ld.param.v2.u64 	{%rl4, %rl5}, [_Z6mul_hiDv3_mS__param_1];
	shr.u64 	%rl9, %rl1, 32;
	and.b64  	%rl10, %rl1, 4294967295;
	shr.u64 	%rl11, %rl4, 32;
	and.b64  	%rl12, %rl4, 4294967295;
	mul.lo.s64 	%rl13, %rl12, %rl9;
	mul.lo.s64 	%rl14, %rl12, %rl10;
	shr.u64 	%rl15, %rl14, 32;
	mad.lo.s64 	%rl16, %rl11, %rl10, %rl15;
	shr.u64 	%rl17, %rl13, 1;
	shr.u64 	%rl18, %rl16, 1;
	add.s64 	%rl19, %rl18, %rl17;
	and.b64  	%rl20, %rl13, %rl16;
	and.b64  	%rl21, %rl20, 1;
	add.s64 	%rl22, %rl19, %rl21;
	shr.u64 	%rl23, %rl22, 31;
	mad.lo.s64 	%rl24, %rl11, %rl9, %rl23;
	shr.u64 	%rl25, %rl2, 32;
	and.b64  	%rl26, %rl2, 4294967295;
	shr.u64 	%rl27, %rl5, 32;
	and.b64  	%rl28, %rl5, 4294967295;
	mul.lo.s64 	%rl29, %rl28, %rl25;
	mul.lo.s64 	%rl30, %rl28, %rl26;
	shr.u64 	%rl31, %rl30, 32;
	mad.lo.s64 	%rl32, %rl27, %rl26, %rl31;
	shr.u64 	%rl33, %rl29, 1;
	shr.u64 	%rl34, %rl32, 1;
	add.s64 	%rl35, %rl34, %rl33;
	and.b64  	%rl36, %rl29, %rl32;
	and.b64  	%rl37, %rl36, 1;
	add.s64 	%rl38, %rl35, %rl37;
	shr.u64 	%rl39, %rl38, 31;
	mad.lo.s64 	%rl40, %rl27, %rl25, %rl39;
	shr.u64 	%rl41, %rl3, 32;
	and.b64  	%rl42, %rl3, 4294967295;
	shr.u64 	%rl43, %rl6, 32;
	and.b64  	%rl44, %rl6, 4294967295;
	mul.lo.s64 	%rl45, %rl44, %rl41;
	mul.lo.s64 	%rl46, %rl44, %rl42;
	shr.u64 	%rl47, %rl46, 32;
	mad.lo.s64 	%rl48, %rl43, %rl42, %rl47;
	shr.u64 	%rl49, %rl45, 1;
	shr.u64 	%rl50, %rl48, 1;
	add.s64 	%rl51, %rl50, %rl49;
	and.b64  	%rl52, %rl45, %rl48;
	and.b64  	%rl53, %rl52, 1;
	add.s64 	%rl54, %rl51, %rl53;
	shr.u64 	%rl55, %rl54, 31;
	mad.lo.s64 	%rl56, %rl43, %rl41, %rl55;
	st.param.v2.b64	[func_retval0+0], {%rl24, %rl40};
	st.param.v2.b64	[func_retval0+16], {%rl56, %rl57};
	ret;
}

	.weak	_Z6mul_hiDv4_mS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z6mul_hiDv4_mS_(
	.param .align 32 .b8 _Z6mul_hiDv4_mS__param_0[32],
	.param .align 32 .b8 _Z6mul_hiDv4_mS__param_1[32]
)
{
	.reg .s64 	%rl<73>;

	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv4_mS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv4_mS__param_0];
	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv4_mS__param_1+16];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv4_mS__param_1];
	shr.u64 	%rl9, %rl1, 32;
	and.b64  	%rl10, %rl1, 4294967295;
	shr.u64 	%rl11, %rl5, 32;
	and.b64  	%rl12, %rl5, 4294967295;
	mul.lo.s64 	%rl13, %rl12, %rl9;
	mul.lo.s64 	%rl14, %rl12, %rl10;
	shr.u64 	%rl15, %rl14, 32;
	mad.lo.s64 	%rl16, %rl11, %rl10, %rl15;
	shr.u64 	%rl17, %rl13, 1;
	shr.u64 	%rl18, %rl16, 1;
	add.s64 	%rl19, %rl18, %rl17;
	and.b64  	%rl20, %rl13, %rl16;
	and.b64  	%rl21, %rl20, 1;
	add.s64 	%rl22, %rl19, %rl21;
	shr.u64 	%rl23, %rl22, 31;
	mad.lo.s64 	%rl24, %rl11, %rl9, %rl23;
	shr.u64 	%rl25, %rl2, 32;
	and.b64  	%rl26, %rl2, 4294967295;
	shr.u64 	%rl27, %rl6, 32;
	and.b64  	%rl28, %rl6, 4294967295;
	mul.lo.s64 	%rl29, %rl28, %rl25;
	mul.lo.s64 	%rl30, %rl28, %rl26;
	shr.u64 	%rl31, %rl30, 32;
	mad.lo.s64 	%rl32, %rl27, %rl26, %rl31;
	shr.u64 	%rl33, %rl29, 1;
	shr.u64 	%rl34, %rl32, 1;
	add.s64 	%rl35, %rl34, %rl33;
	and.b64  	%rl36, %rl29, %rl32;
	and.b64  	%rl37, %rl36, 1;
	add.s64 	%rl38, %rl35, %rl37;
	shr.u64 	%rl39, %rl38, 31;
	mad.lo.s64 	%rl40, %rl27, %rl25, %rl39;
	shr.u64 	%rl41, %rl3, 32;
	and.b64  	%rl42, %rl3, 4294967295;
	shr.u64 	%rl43, %rl7, 32;
	and.b64  	%rl44, %rl7, 4294967295;
	mul.lo.s64 	%rl45, %rl44, %rl41;
	mul.lo.s64 	%rl46, %rl44, %rl42;
	shr.u64 	%rl47, %rl46, 32;
	mad.lo.s64 	%rl48, %rl43, %rl42, %rl47;
	shr.u64 	%rl49, %rl45, 1;
	shr.u64 	%rl50, %rl48, 1;
	add.s64 	%rl51, %rl50, %rl49;
	and.b64  	%rl52, %rl45, %rl48;
	and.b64  	%rl53, %rl52, 1;
	add.s64 	%rl54, %rl51, %rl53;
	shr.u64 	%rl55, %rl54, 31;
	mad.lo.s64 	%rl56, %rl43, %rl41, %rl55;
	shr.u64 	%rl57, %rl4, 32;
	and.b64  	%rl58, %rl4, 4294967295;
	shr.u64 	%rl59, %rl8, 32;
	and.b64  	%rl60, %rl8, 4294967295;
	mul.lo.s64 	%rl61, %rl60, %rl57;
	mul.lo.s64 	%rl62, %rl60, %rl58;
	shr.u64 	%rl63, %rl62, 32;
	mad.lo.s64 	%rl64, %rl59, %rl58, %rl63;
	shr.u64 	%rl65, %rl61, 1;
	shr.u64 	%rl66, %rl64, 1;
	add.s64 	%rl67, %rl66, %rl65;
	and.b64  	%rl68, %rl61, %rl64;
	and.b64  	%rl69, %rl68, 1;
	add.s64 	%rl70, %rl67, %rl69;
	shr.u64 	%rl71, %rl70, 31;
	mad.lo.s64 	%rl72, %rl59, %rl57, %rl71;
	st.param.v2.b64	[func_retval0+0], {%rl24, %rl40};
	st.param.v2.b64	[func_retval0+16], {%rl56, %rl72};
	ret;
}

	.weak	_Z6mul_hiDv8_mS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z6mul_hiDv8_mS_(
	.param .align 64 .b8 _Z6mul_hiDv8_mS__param_0[64],
	.param .align 64 .b8 _Z6mul_hiDv8_mS__param_1[64]
)
{
	.reg .s64 	%rl<145>;

	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv8_mS__param_0+48];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv8_mS__param_0+32];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv8_mS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv8_mS__param_0];
	ld.param.v2.u64 	{%rl15, %rl16}, [_Z6mul_hiDv8_mS__param_1+48];
	ld.param.v2.u64 	{%rl13, %rl14}, [_Z6mul_hiDv8_mS__param_1+32];
	ld.param.v2.u64 	{%rl11, %rl12}, [_Z6mul_hiDv8_mS__param_1+16];
	ld.param.v2.u64 	{%rl9, %rl10}, [_Z6mul_hiDv8_mS__param_1];
	shr.u64 	%rl17, %rl1, 32;
	and.b64  	%rl18, %rl1, 4294967295;
	shr.u64 	%rl19, %rl9, 32;
	and.b64  	%rl20, %rl9, 4294967295;
	mul.lo.s64 	%rl21, %rl20, %rl17;
	mul.lo.s64 	%rl22, %rl20, %rl18;
	shr.u64 	%rl23, %rl22, 32;
	mad.lo.s64 	%rl24, %rl19, %rl18, %rl23;
	shr.u64 	%rl25, %rl21, 1;
	shr.u64 	%rl26, %rl24, 1;
	add.s64 	%rl27, %rl26, %rl25;
	and.b64  	%rl28, %rl21, %rl24;
	and.b64  	%rl29, %rl28, 1;
	add.s64 	%rl30, %rl27, %rl29;
	shr.u64 	%rl31, %rl30, 31;
	mad.lo.s64 	%rl32, %rl19, %rl17, %rl31;
	shr.u64 	%rl33, %rl2, 32;
	and.b64  	%rl34, %rl2, 4294967295;
	shr.u64 	%rl35, %rl10, 32;
	and.b64  	%rl36, %rl10, 4294967295;
	mul.lo.s64 	%rl37, %rl36, %rl33;
	mul.lo.s64 	%rl38, %rl36, %rl34;
	shr.u64 	%rl39, %rl38, 32;
	mad.lo.s64 	%rl40, %rl35, %rl34, %rl39;
	shr.u64 	%rl41, %rl37, 1;
	shr.u64 	%rl42, %rl40, 1;
	add.s64 	%rl43, %rl42, %rl41;
	and.b64  	%rl44, %rl37, %rl40;
	and.b64  	%rl45, %rl44, 1;
	add.s64 	%rl46, %rl43, %rl45;
	shr.u64 	%rl47, %rl46, 31;
	mad.lo.s64 	%rl48, %rl35, %rl33, %rl47;
	shr.u64 	%rl49, %rl3, 32;
	and.b64  	%rl50, %rl3, 4294967295;
	shr.u64 	%rl51, %rl11, 32;
	and.b64  	%rl52, %rl11, 4294967295;
	mul.lo.s64 	%rl53, %rl52, %rl49;
	mul.lo.s64 	%rl54, %rl52, %rl50;
	shr.u64 	%rl55, %rl54, 32;
	mad.lo.s64 	%rl56, %rl51, %rl50, %rl55;
	shr.u64 	%rl57, %rl53, 1;
	shr.u64 	%rl58, %rl56, 1;
	add.s64 	%rl59, %rl58, %rl57;
	and.b64  	%rl60, %rl53, %rl56;
	and.b64  	%rl61, %rl60, 1;
	add.s64 	%rl62, %rl59, %rl61;
	shr.u64 	%rl63, %rl62, 31;
	mad.lo.s64 	%rl64, %rl51, %rl49, %rl63;
	shr.u64 	%rl65, %rl4, 32;
	and.b64  	%rl66, %rl4, 4294967295;
	shr.u64 	%rl67, %rl12, 32;
	and.b64  	%rl68, %rl12, 4294967295;
	mul.lo.s64 	%rl69, %rl68, %rl65;
	mul.lo.s64 	%rl70, %rl68, %rl66;
	shr.u64 	%rl71, %rl70, 32;
	mad.lo.s64 	%rl72, %rl67, %rl66, %rl71;
	shr.u64 	%rl73, %rl69, 1;
	shr.u64 	%rl74, %rl72, 1;
	add.s64 	%rl75, %rl74, %rl73;
	and.b64  	%rl76, %rl69, %rl72;
	and.b64  	%rl77, %rl76, 1;
	add.s64 	%rl78, %rl75, %rl77;
	shr.u64 	%rl79, %rl78, 31;
	mad.lo.s64 	%rl80, %rl67, %rl65, %rl79;
	shr.u64 	%rl81, %rl5, 32;
	and.b64  	%rl82, %rl5, 4294967295;
	shr.u64 	%rl83, %rl13, 32;
	and.b64  	%rl84, %rl13, 4294967295;
	mul.lo.s64 	%rl85, %rl84, %rl81;
	mul.lo.s64 	%rl86, %rl84, %rl82;
	shr.u64 	%rl87, %rl86, 32;
	mad.lo.s64 	%rl88, %rl83, %rl82, %rl87;
	shr.u64 	%rl89, %rl85, 1;
	shr.u64 	%rl90, %rl88, 1;
	add.s64 	%rl91, %rl90, %rl89;
	and.b64  	%rl92, %rl85, %rl88;
	and.b64  	%rl93, %rl92, 1;
	add.s64 	%rl94, %rl91, %rl93;
	shr.u64 	%rl95, %rl94, 31;
	mad.lo.s64 	%rl96, %rl83, %rl81, %rl95;
	shr.u64 	%rl97, %rl6, 32;
	and.b64  	%rl98, %rl6, 4294967295;
	shr.u64 	%rl99, %rl14, 32;
	and.b64  	%rl100, %rl14, 4294967295;
	mul.lo.s64 	%rl101, %rl100, %rl97;
	mul.lo.s64 	%rl102, %rl100, %rl98;
	shr.u64 	%rl103, %rl102, 32;
	mad.lo.s64 	%rl104, %rl99, %rl98, %rl103;
	shr.u64 	%rl105, %rl101, 1;
	shr.u64 	%rl106, %rl104, 1;
	add.s64 	%rl107, %rl106, %rl105;
	and.b64  	%rl108, %rl101, %rl104;
	and.b64  	%rl109, %rl108, 1;
	add.s64 	%rl110, %rl107, %rl109;
	shr.u64 	%rl111, %rl110, 31;
	mad.lo.s64 	%rl112, %rl99, %rl97, %rl111;
	shr.u64 	%rl113, %rl7, 32;
	and.b64  	%rl114, %rl7, 4294967295;
	shr.u64 	%rl115, %rl15, 32;
	and.b64  	%rl116, %rl15, 4294967295;
	mul.lo.s64 	%rl117, %rl116, %rl113;
	mul.lo.s64 	%rl118, %rl116, %rl114;
	shr.u64 	%rl119, %rl118, 32;
	mad.lo.s64 	%rl120, %rl115, %rl114, %rl119;
	shr.u64 	%rl121, %rl117, 1;
	shr.u64 	%rl122, %rl120, 1;
	add.s64 	%rl123, %rl122, %rl121;
	and.b64  	%rl124, %rl117, %rl120;
	and.b64  	%rl125, %rl124, 1;
	add.s64 	%rl126, %rl123, %rl125;
	shr.u64 	%rl127, %rl126, 31;
	mad.lo.s64 	%rl128, %rl115, %rl113, %rl127;
	shr.u64 	%rl129, %rl8, 32;
	and.b64  	%rl130, %rl8, 4294967295;
	shr.u64 	%rl131, %rl16, 32;
	and.b64  	%rl132, %rl16, 4294967295;
	mul.lo.s64 	%rl133, %rl132, %rl129;
	mul.lo.s64 	%rl134, %rl132, %rl130;
	shr.u64 	%rl135, %rl134, 32;
	mad.lo.s64 	%rl136, %rl131, %rl130, %rl135;
	shr.u64 	%rl137, %rl133, 1;
	shr.u64 	%rl138, %rl136, 1;
	add.s64 	%rl139, %rl138, %rl137;
	and.b64  	%rl140, %rl133, %rl136;
	and.b64  	%rl141, %rl140, 1;
	add.s64 	%rl142, %rl139, %rl141;
	shr.u64 	%rl143, %rl142, 31;
	mad.lo.s64 	%rl144, %rl131, %rl129, %rl143;
	st.param.v2.b64	[func_retval0+0], {%rl32, %rl48};
	st.param.v2.b64	[func_retval0+16], {%rl64, %rl80};
	st.param.v2.b64	[func_retval0+32], {%rl96, %rl112};
	st.param.v2.b64	[func_retval0+48], {%rl128, %rl144};
	ret;
}

	.weak	_Z6mul_hiDv16_mS_
.func  (.param .align 128 .b8 func_retval0[128]) _Z6mul_hiDv16_mS_(
	.param .align 128 .b8 _Z6mul_hiDv16_mS__param_0[128],
	.param .align 128 .b8 _Z6mul_hiDv16_mS__param_1[128]
)
{
	.reg .s64 	%rl<49>;

	ld.param.v2.u64 	{%rl15, %rl16}, [_Z6mul_hiDv16_mS__param_0+112];
	ld.param.v2.u64 	{%rl13, %rl14}, [_Z6mul_hiDv16_mS__param_0+96];
	ld.param.v2.u64 	{%rl11, %rl12}, [_Z6mul_hiDv16_mS__param_0+80];
	ld.param.v2.u64 	{%rl9, %rl10}, [_Z6mul_hiDv16_mS__param_0+64];
	ld.param.v2.u64 	{%rl7, %rl8}, [_Z6mul_hiDv16_mS__param_0+48];
	ld.param.v2.u64 	{%rl5, %rl6}, [_Z6mul_hiDv16_mS__param_0+32];
	ld.param.v2.u64 	{%rl3, %rl4}, [_Z6mul_hiDv16_mS__param_0+16];
	ld.param.v2.u64 	{%rl1, %rl2}, [_Z6mul_hiDv16_mS__param_0];
	ld.param.v2.u64 	{%rl31, %rl32}, [_Z6mul_hiDv16_mS__param_1+112];
	ld.param.v2.u64 	{%rl29, %rl30}, [_Z6mul_hiDv16_mS__param_1+96];
	ld.param.v2.u64 	{%rl27, %rl28}, [_Z6mul_hiDv16_mS__param_1+80];
	ld.param.v2.u64 	{%rl25, %rl26}, [_Z6mul_hiDv16_mS__param_1+64];
	ld.param.v2.u64 	{%rl23, %rl24}, [_Z6mul_hiDv16_mS__param_1+48];
	ld.param.v2.u64 	{%rl21, %rl22}, [_Z6mul_hiDv16_mS__param_1+32];
	ld.param.v2.u64 	{%rl19, %rl20}, [_Z6mul_hiDv16_mS__param_1+16];
	ld.param.v2.u64 	{%rl17, %rl18}, [_Z6mul_hiDv16_mS__param_1];
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .align 64 .b8 param0[64];
	st.param.v2.b64	[param0+0], {%rl1, %rl2};
	st.param.v2.b64	[param0+16], {%rl3, %rl4};
	st.param.v2.b64	[param0+32], {%rl5, %rl6};
	st.param.v2.b64	[param0+48], {%rl7, %rl8};
	.param .align 64 .b8 param1[64];
	st.param.v2.b64	[param1+0], {%rl17, %rl18};
	st.param.v2.b64	[param1+16], {%rl19, %rl20};
	st.param.v2.b64	[param1+32], {%rl21, %rl22};
	st.param.v2.b64	[param1+48], {%rl23, %rl24};
	.param .align 64 .b8 retval0[64];
	call.uni (retval0), 
	_Z6mul_hiDv8_mS_, 
	(
	param0, 
	param1
	);
	ld.param.v2.b64	{%rl33, %rl34}, [retval0+0];
	ld.param.v2.b64	{%rl35, %rl36}, [retval0+16];
	ld.param.v2.b64	{%rl37, %rl38}, [retval0+32];
	ld.param.v2.b64	{%rl39, %rl40}, [retval0+48];
	
	//{
	}// Callseq End 2
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .align 64 .b8 param0[64];
	st.param.v2.b64	[param0+0], {%rl9, %rl10};
	st.param.v2.b64	[param0+16], {%rl11, %rl12};
	st.param.v2.b64	[param0+32], {%rl13, %rl14};
	st.param.v2.b64	[param0+48], {%rl15, %rl16};
	.param .align 64 .b8 param1[64];
	st.param.v2.b64	[param1+0], {%rl25, %rl26};
	st.param.v2.b64	[param1+16], {%rl27, %rl28};
	st.param.v2.b64	[param1+32], {%rl29, %rl30};
	st.param.v2.b64	[param1+48], {%rl31, %rl32};
	.param .align 64 .b8 retval0[64];
	call.uni (retval0), 
	_Z6mul_hiDv8_mS_, 
	(
	param0, 
	param1
	);
	ld.param.v2.b64	{%rl41, %rl42}, [retval0+0];
	ld.param.v2.b64	{%rl43, %rl44}, [retval0+16];
	ld.param.v2.b64	{%rl45, %rl46}, [retval0+32];
	ld.param.v2.b64	{%rl47, %rl48}, [retval0+48];
	
	//{
	}// Callseq End 3
	st.param.v2.b64	[func_retval0+0], {%rl33, %rl34};
	st.param.v2.b64	[func_retval0+16], {%rl35, %rl36};
	st.param.v2.b64	[func_retval0+32], {%rl37, %rl38};
	st.param.v2.b64	[func_retval0+48], {%rl39, %rl40};
	st.param.v2.b64	[func_retval0+64], {%rl41, %rl42};
	st.param.v2.b64	[func_retval0+80], {%rl43, %rl44};
	st.param.v2.b64	[func_retval0+96], {%rl45, %rl46};
	st.param.v2.b64	[func_retval0+112], {%rl47, %rl48};
	ret;
}

	.weak	_Z8upsamplech
.func  (.param .b32 func_retval0) _Z8upsamplech(
	.param .b32 _Z8upsamplech_param_0,
	.param .b32 _Z8upsamplech_param_1
)
{
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<2>;

	ld.param.u8 	%rs1, [_Z8upsamplech_param_0];
	ld.param.u8 	%rs2, [_Z8upsamplech_param_1];
	shl.b16 	%rs3, %rs1, 8;
	or.b16  	%rs4, %rs3, %rs2;
	cvt.u32.u16	%r1, %rs4;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	.weak	_Z8upsampleDv2_cDv2_h
.func  (.param .align 4 .b8 func_retval0[4]) _Z8upsampleDv2_cDv2_h(
	.param .align 2 .b8 _Z8upsampleDv2_cDv2_h_param_0[2],
	.param .align 2 .b8 _Z8upsampleDv2_cDv2_h_param_1[2]
)
{
	.reg .s16 	%rs<11>;

	ld.param.v2.u8 	{%rs1, %rs2}, [_Z8upsampleDv2_cDv2_h_param_0];
	ld.param.v2.u8 	{%rs3, %rs4}, [_Z8upsampleDv2_cDv2_h_param_1];
	shl.b16 	%rs5, %rs1, 8;
	and.b16  	%rs6, %rs3, 255;
	or.b16  	%rs7, %rs5, %rs6;
	shl.b16 	%rs8, %rs2, 8;
	and.b16  	%rs9, %rs4, 255;
	or.b16  	%rs10, %rs8, %rs9;
	st.param.v2.b16	[func_retval0+0], {%rs7, %rs10};
	ret;
}

	.weak	_Z8upsampleDv3_cDv3_h
.func  (.param .align 8 .b8 func_retval0[6]) _Z8upsampleDv3_cDv3_h(
	.param .align 4 .b8 _Z8upsampleDv3_cDv3_h_param_0[4],
	.param .align 4 .b8 _Z8upsampleDv3_cDv3_h_param_1[4]
)
{
	.reg .s16 	%rs<19>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs7}, [_Z8upsampleDv3_cDv3_h_param_0];
	ld.param.v4.u8 	{%rs4, %rs5, %rs6, %rs8}, [_Z8upsampleDv3_cDv3_h_param_1];
	shl.b16 	%rs9, %rs1, 8;
	and.b16  	%rs10, %rs4, 255;
	or.b16  	%rs11, %rs9, %rs10;
	shl.b16 	%rs12, %rs2, 8;
	and.b16  	%rs13, %rs5, 255;
	or.b16  	%rs14, %rs12, %rs13;
	shl.b16 	%rs15, %rs3, 8;
	and.b16  	%rs16, %rs6, 255;
	or.b16  	%rs17, %rs15, %rs16;
	st.param.v4.b16	[func_retval0+0], {%rs11, %rs14, %rs17, %rs18};
	ret;
}

	.weak	_Z8upsampleDv4_cDv4_h
.func  (.param .align 8 .b8 func_retval0[8]) _Z8upsampleDv4_cDv4_h(
	.param .align 4 .b8 _Z8upsampleDv4_cDv4_h_param_0[4],
	.param .align 4 .b8 _Z8upsampleDv4_cDv4_h_param_1[4]
)
{
	.reg .s16 	%rs<21>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv4_cDv4_h_param_0];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv4_cDv4_h_param_1];
	shl.b16 	%rs9, %rs1, 8;
	and.b16  	%rs10, %rs5, 255;
	or.b16  	%rs11, %rs9, %rs10;
	shl.b16 	%rs12, %rs2, 8;
	and.b16  	%rs13, %rs6, 255;
	or.b16  	%rs14, %rs12, %rs13;
	shl.b16 	%rs15, %rs3, 8;
	and.b16  	%rs16, %rs7, 255;
	or.b16  	%rs17, %rs15, %rs16;
	shl.b16 	%rs18, %rs4, 8;
	and.b16  	%rs19, %rs8, 255;
	or.b16  	%rs20, %rs18, %rs19;
	st.param.v4.b16	[func_retval0+0], {%rs11, %rs14, %rs17, %rs20};
	ret;
}

	.weak	_Z8upsampleDv8_cDv8_h
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv8_cDv8_h(
	.param .align 8 .b8 _Z8upsampleDv8_cDv8_h_param_0[8],
	.param .align 8 .b8 _Z8upsampleDv8_cDv8_h_param_1[8]
)
{
	.reg .s16 	%rs<41>;

	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv8_cDv8_h_param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv8_cDv8_h_param_0];
	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv8_cDv8_h_param_1+4];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv8_cDv8_h_param_1];
	shl.b16 	%rs17, %rs1, 8;
	and.b16  	%rs18, %rs9, 255;
	or.b16  	%rs19, %rs17, %rs18;
	shl.b16 	%rs20, %rs2, 8;
	and.b16  	%rs21, %rs10, 255;
	or.b16  	%rs22, %rs20, %rs21;
	shl.b16 	%rs23, %rs3, 8;
	and.b16  	%rs24, %rs11, 255;
	or.b16  	%rs25, %rs23, %rs24;
	shl.b16 	%rs26, %rs4, 8;
	and.b16  	%rs27, %rs12, 255;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs5, 8;
	and.b16  	%rs30, %rs13, 255;
	or.b16  	%rs31, %rs29, %rs30;
	shl.b16 	%rs32, %rs6, 8;
	and.b16  	%rs33, %rs14, 255;
	or.b16  	%rs34, %rs32, %rs33;
	shl.b16 	%rs35, %rs7, 8;
	and.b16  	%rs36, %rs15, 255;
	or.b16  	%rs37, %rs35, %rs36;
	shl.b16 	%rs38, %rs8, 8;
	and.b16  	%rs39, %rs16, 255;
	or.b16  	%rs40, %rs38, %rs39;
	st.param.v4.b16	[func_retval0+0], {%rs19, %rs22, %rs25, %rs28};
	st.param.v4.b16	[func_retval0+8], {%rs31, %rs34, %rs37, %rs40};
	ret;
}

	.weak	_Z8upsampleDv16_cDv16_h
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv16_cDv16_h(
	.param .align 16 .b8 _Z8upsampleDv16_cDv16_h_param_0[16],
	.param .align 16 .b8 _Z8upsampleDv16_cDv16_h_param_1[16]
)
{
	.reg .s16 	%rs<81>;

	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv16_cDv16_h_param_0+12];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv16_cDv16_h_param_0+8];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv16_cDv16_h_param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv16_cDv16_h_param_0];
	ld.param.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [_Z8upsampleDv16_cDv16_h_param_1+12];
	ld.param.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [_Z8upsampleDv16_cDv16_h_param_1+8];
	ld.param.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [_Z8upsampleDv16_cDv16_h_param_1+4];
	ld.param.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [_Z8upsampleDv16_cDv16_h_param_1];
	shl.b16 	%rs33, %rs1, 8;
	and.b16  	%rs34, %rs17, 255;
	or.b16  	%rs35, %rs33, %rs34;
	shl.b16 	%rs36, %rs2, 8;
	and.b16  	%rs37, %rs18, 255;
	or.b16  	%rs38, %rs36, %rs37;
	shl.b16 	%rs39, %rs3, 8;
	and.b16  	%rs40, %rs19, 255;
	or.b16  	%rs41, %rs39, %rs40;
	shl.b16 	%rs42, %rs4, 8;
	and.b16  	%rs43, %rs20, 255;
	or.b16  	%rs44, %rs42, %rs43;
	shl.b16 	%rs45, %rs5, 8;
	and.b16  	%rs46, %rs21, 255;
	or.b16  	%rs47, %rs45, %rs46;
	shl.b16 	%rs48, %rs6, 8;
	and.b16  	%rs49, %rs22, 255;
	or.b16  	%rs50, %rs48, %rs49;
	shl.b16 	%rs51, %rs7, 8;
	and.b16  	%rs52, %rs23, 255;
	or.b16  	%rs53, %rs51, %rs52;
	shl.b16 	%rs54, %rs8, 8;
	and.b16  	%rs55, %rs24, 255;
	or.b16  	%rs56, %rs54, %rs55;
	shl.b16 	%rs57, %rs9, 8;
	and.b16  	%rs58, %rs25, 255;
	or.b16  	%rs59, %rs57, %rs58;
	shl.b16 	%rs60, %rs10, 8;
	and.b16  	%rs61, %rs26, 255;
	or.b16  	%rs62, %rs60, %rs61;
	shl.b16 	%rs63, %rs11, 8;
	and.b16  	%rs64, %rs27, 255;
	or.b16  	%rs65, %rs63, %rs64;
	shl.b16 	%rs66, %rs12, 8;
	and.b16  	%rs67, %rs28, 255;
	or.b16  	%rs68, %rs66, %rs67;
	shl.b16 	%rs69, %rs13, 8;
	and.b16  	%rs70, %rs29, 255;
	or.b16  	%rs71, %rs69, %rs70;
	shl.b16 	%rs72, %rs14, 8;
	and.b16  	%rs73, %rs30, 255;
	or.b16  	%rs74, %rs72, %rs73;
	shl.b16 	%rs75, %rs15, 8;
	and.b16  	%rs76, %rs31, 255;
	or.b16  	%rs77, %rs75, %rs76;
	shl.b16 	%rs78, %rs16, 8;
	and.b16  	%rs79, %rs32, 255;
	or.b16  	%rs80, %rs78, %rs79;
	st.param.v4.b16	[func_retval0+0], {%rs35, %rs38, %rs41, %rs44};
	st.param.v4.b16	[func_retval0+8], {%rs47, %rs50, %rs53, %rs56};
	st.param.v4.b16	[func_retval0+16], {%rs59, %rs62, %rs65, %rs68};
	st.param.v4.b16	[func_retval0+24], {%rs71, %rs74, %rs77, %rs80};
	ret;
}

	.weak	_Z8upsamplehh
.func  (.param .b32 func_retval0) _Z8upsamplehh(
	.param .b32 _Z8upsamplehh_param_0,
	.param .b32 _Z8upsamplehh_param_1
)
{
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<2>;

	ld.param.u8 	%rs1, [_Z8upsamplehh_param_0];
	ld.param.u8 	%rs2, [_Z8upsamplehh_param_1];
	shl.b16 	%rs3, %rs1, 8;
	or.b16  	%rs4, %rs3, %rs2;
	cvt.u32.u16	%r1, %rs4;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

	.weak	_Z8upsampleDv2_hS_
.func  (.param .align 4 .b8 func_retval0[4]) _Z8upsampleDv2_hS_(
	.param .align 2 .b8 _Z8upsampleDv2_hS__param_0[2],
	.param .align 2 .b8 _Z8upsampleDv2_hS__param_1[2]
)
{
	.reg .s16 	%rs<11>;

	ld.param.v2.u8 	{%rs1, %rs2}, [_Z8upsampleDv2_hS__param_0];
	ld.param.v2.u8 	{%rs3, %rs4}, [_Z8upsampleDv2_hS__param_1];
	shl.b16 	%rs5, %rs1, 8;
	and.b16  	%rs6, %rs3, 255;
	or.b16  	%rs7, %rs5, %rs6;
	shl.b16 	%rs8, %rs2, 8;
	and.b16  	%rs9, %rs4, 255;
	or.b16  	%rs10, %rs8, %rs9;
	st.param.v2.b16	[func_retval0+0], {%rs7, %rs10};
	ret;
}

	.weak	_Z8upsampleDv3_hS_
.func  (.param .align 8 .b8 func_retval0[6]) _Z8upsampleDv3_hS_(
	.param .align 4 .b8 _Z8upsampleDv3_hS__param_0[4],
	.param .align 4 .b8 _Z8upsampleDv3_hS__param_1[4]
)
{
	.reg .s16 	%rs<19>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs7}, [_Z8upsampleDv3_hS__param_0];
	ld.param.v4.u8 	{%rs4, %rs5, %rs6, %rs8}, [_Z8upsampleDv3_hS__param_1];
	shl.b16 	%rs9, %rs1, 8;
	and.b16  	%rs10, %rs4, 255;
	or.b16  	%rs11, %rs9, %rs10;
	shl.b16 	%rs12, %rs2, 8;
	and.b16  	%rs13, %rs5, 255;
	or.b16  	%rs14, %rs12, %rs13;
	shl.b16 	%rs15, %rs3, 8;
	and.b16  	%rs16, %rs6, 255;
	or.b16  	%rs17, %rs15, %rs16;
	st.param.v4.b16	[func_retval0+0], {%rs11, %rs14, %rs17, %rs18};
	ret;
}

	.weak	_Z8upsampleDv4_hS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z8upsampleDv4_hS_(
	.param .align 4 .b8 _Z8upsampleDv4_hS__param_0[4],
	.param .align 4 .b8 _Z8upsampleDv4_hS__param_1[4]
)
{
	.reg .s16 	%rs<21>;

	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv4_hS__param_0];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv4_hS__param_1];
	shl.b16 	%rs9, %rs1, 8;
	and.b16  	%rs10, %rs5, 255;
	or.b16  	%rs11, %rs9, %rs10;
	shl.b16 	%rs12, %rs2, 8;
	and.b16  	%rs13, %rs6, 255;
	or.b16  	%rs14, %rs12, %rs13;
	shl.b16 	%rs15, %rs3, 8;
	and.b16  	%rs16, %rs7, 255;
	or.b16  	%rs17, %rs15, %rs16;
	shl.b16 	%rs18, %rs4, 8;
	and.b16  	%rs19, %rs8, 255;
	or.b16  	%rs20, %rs18, %rs19;
	st.param.v4.b16	[func_retval0+0], {%rs11, %rs14, %rs17, %rs20};
	ret;
}

	.weak	_Z8upsampleDv8_hS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv8_hS_(
	.param .align 8 .b8 _Z8upsampleDv8_hS__param_0[8],
	.param .align 8 .b8 _Z8upsampleDv8_hS__param_1[8]
)
{
	.reg .s16 	%rs<41>;

	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv8_hS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv8_hS__param_0];
	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv8_hS__param_1+4];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv8_hS__param_1];
	shl.b16 	%rs17, %rs1, 8;
	and.b16  	%rs18, %rs9, 255;
	or.b16  	%rs19, %rs17, %rs18;
	shl.b16 	%rs20, %rs2, 8;
	and.b16  	%rs21, %rs10, 255;
	or.b16  	%rs22, %rs20, %rs21;
	shl.b16 	%rs23, %rs3, 8;
	and.b16  	%rs24, %rs11, 255;
	or.b16  	%rs25, %rs23, %rs24;
	shl.b16 	%rs26, %rs4, 8;
	and.b16  	%rs27, %rs12, 255;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs5, 8;
	and.b16  	%rs30, %rs13, 255;
	or.b16  	%rs31, %rs29, %rs30;
	shl.b16 	%rs32, %rs6, 8;
	and.b16  	%rs33, %rs14, 255;
	or.b16  	%rs34, %rs32, %rs33;
	shl.b16 	%rs35, %rs7, 8;
	and.b16  	%rs36, %rs15, 255;
	or.b16  	%rs37, %rs35, %rs36;
	shl.b16 	%rs38, %rs8, 8;
	and.b16  	%rs39, %rs16, 255;
	or.b16  	%rs40, %rs38, %rs39;
	st.param.v4.b16	[func_retval0+0], {%rs19, %rs22, %rs25, %rs28};
	st.param.v4.b16	[func_retval0+8], {%rs31, %rs34, %rs37, %rs40};
	ret;
}

	.weak	_Z8upsampleDv16_hS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv16_hS_(
	.param .align 16 .b8 _Z8upsampleDv16_hS__param_0[16],
	.param .align 16 .b8 _Z8upsampleDv16_hS__param_1[16]
)
{
	.reg .s16 	%rs<81>;

	ld.param.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv16_hS__param_0+12];
	ld.param.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv16_hS__param_0+8];
	ld.param.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv16_hS__param_0+4];
	ld.param.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv16_hS__param_0];
	ld.param.v4.u8 	{%rs29, %rs30, %rs31, %rs32}, [_Z8upsampleDv16_hS__param_1+12];
	ld.param.v4.u8 	{%rs25, %rs26, %rs27, %rs28}, [_Z8upsampleDv16_hS__param_1+8];
	ld.param.v4.u8 	{%rs21, %rs22, %rs23, %rs24}, [_Z8upsampleDv16_hS__param_1+4];
	ld.param.v4.u8 	{%rs17, %rs18, %rs19, %rs20}, [_Z8upsampleDv16_hS__param_1];
	shl.b16 	%rs33, %rs1, 8;
	and.b16  	%rs34, %rs17, 255;
	or.b16  	%rs35, %rs33, %rs34;
	shl.b16 	%rs36, %rs2, 8;
	and.b16  	%rs37, %rs18, 255;
	or.b16  	%rs38, %rs36, %rs37;
	shl.b16 	%rs39, %rs3, 8;
	and.b16  	%rs40, %rs19, 255;
	or.b16  	%rs41, %rs39, %rs40;
	shl.b16 	%rs42, %rs4, 8;
	and.b16  	%rs43, %rs20, 255;
	or.b16  	%rs44, %rs42, %rs43;
	shl.b16 	%rs45, %rs5, 8;
	and.b16  	%rs46, %rs21, 255;
	or.b16  	%rs47, %rs45, %rs46;
	shl.b16 	%rs48, %rs6, 8;
	and.b16  	%rs49, %rs22, 255;
	or.b16  	%rs50, %rs48, %rs49;
	shl.b16 	%rs51, %rs7, 8;
	and.b16  	%rs52, %rs23, 255;
	or.b16  	%rs53, %rs51, %rs52;
	shl.b16 	%rs54, %rs8, 8;
	and.b16  	%rs55, %rs24, 255;
	or.b16  	%rs56, %rs54, %rs55;
	shl.b16 	%rs57, %rs9, 8;
	and.b16  	%rs58, %rs25, 255;
	or.b16  	%rs59, %rs57, %rs58;
	shl.b16 	%rs60, %rs10, 8;
	and.b16  	%rs61, %rs26, 255;
	or.b16  	%rs62, %rs60, %rs61;
	shl.b16 	%rs63, %rs11, 8;
	and.b16  	%rs64, %rs27, 255;
	or.b16  	%rs65, %rs63, %rs64;
	shl.b16 	%rs66, %rs12, 8;
	and.b16  	%rs67, %rs28, 255;
	or.b16  	%rs68, %rs66, %rs67;
	shl.b16 	%rs69, %rs13, 8;
	and.b16  	%rs70, %rs29, 255;
	or.b16  	%rs71, %rs69, %rs70;
	shl.b16 	%rs72, %rs14, 8;
	and.b16  	%rs73, %rs30, 255;
	or.b16  	%rs74, %rs72, %rs73;
	shl.b16 	%rs75, %rs15, 8;
	and.b16  	%rs76, %rs31, 255;
	or.b16  	%rs77, %rs75, %rs76;
	shl.b16 	%rs78, %rs16, 8;
	and.b16  	%rs79, %rs32, 255;
	or.b16  	%rs80, %rs78, %rs79;
	st.param.v4.b16	[func_retval0+0], {%rs35, %rs38, %rs41, %rs44};
	st.param.v4.b16	[func_retval0+8], {%rs47, %rs50, %rs53, %rs56};
	st.param.v4.b16	[func_retval0+16], {%rs59, %rs62, %rs65, %rs68};
	st.param.v4.b16	[func_retval0+24], {%rs71, %rs74, %rs77, %rs80};
	ret;
}

	.weak	_Z8upsamplest
.func  (.param .b32 func_retval0) _Z8upsamplest(
	.param .b32 _Z8upsamplest_param_0,
	.param .b32 _Z8upsamplest_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;

	ld.param.u16 	%rs1, [_Z8upsamplest_param_0];
	ld.param.u16 	%rs2, [_Z8upsamplest_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs2;
	or.b32  	%r4, %r2, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	.weak	_Z8upsampleDv2_sDv2_t
.func  (.param .align 8 .b8 func_retval0[8]) _Z8upsampleDv2_sDv2_t(
	.param .align 4 .b8 _Z8upsampleDv2_sDv2_t_param_0[4],
	.param .align 4 .b8 _Z8upsampleDv2_sDv2_t_param_1[4]
)
{
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<9>;

	ld.param.v2.u16 	{%rs1, %rs2}, [_Z8upsampleDv2_sDv2_t_param_0];
	ld.param.v2.u16 	{%rs3, %rs4}, [_Z8upsampleDv2_sDv2_t_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs3;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs4;
	or.b32  	%r8, %r6, %r7;
	st.param.v2.b32	[func_retval0+0], {%r4, %r8};
	ret;
}

	.weak	_Z8upsampleDv3_sDv3_t
.func  (.param .align 16 .b8 func_retval0[12]) _Z8upsampleDv3_sDv3_t(
	.param .align 8 .b8 _Z8upsampleDv3_sDv3_t_param_0[8],
	.param .align 8 .b8 _Z8upsampleDv3_sDv3_t_param_1[8]
)
{
	.reg .s16 	%rs<9>;
	.reg .s32 	%r<14>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs7}, [_Z8upsampleDv3_sDv3_t_param_0];
	ld.param.v4.u16 	{%rs4, %rs5, %rs6, %rs8}, [_Z8upsampleDv3_sDv3_t_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs4;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs5;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs6;
	or.b32  	%r12, %r10, %r11;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r13};
	ret;
}

	.weak	_Z8upsampleDv4_sDv4_t
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv4_sDv4_t(
	.param .align 8 .b8 _Z8upsampleDv4_sDv4_t_param_0[8],
	.param .align 8 .b8 _Z8upsampleDv4_sDv4_t_param_1[8]
)
{
	.reg .s16 	%rs<9>;
	.reg .s32 	%r<17>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv4_sDv4_t_param_0];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv4_sDv4_t_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs5;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs6;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs7;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs8;
	or.b32  	%r16, %r14, %r15;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	ret;
}

	.weak	_Z8upsampleDv8_sDv8_t
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv8_sDv8_t(
	.param .align 16 .b8 _Z8upsampleDv8_sDv8_t_param_0[16],
	.param .align 16 .b8 _Z8upsampleDv8_sDv8_t_param_1[16]
)
{
	.reg .s16 	%rs<17>;
	.reg .s32 	%r<33>;

	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv8_sDv8_t_param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv8_sDv8_t_param_0];
	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv8_sDv8_t_param_1+8];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv8_sDv8_t_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs9;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs10;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs11;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs12;
	or.b32  	%r16, %r14, %r15;
	cvt.u32.u16	%r17, %rs5;
	shl.b32 	%r18, %r17, 16;
	cvt.u32.u16	%r19, %rs13;
	or.b32  	%r20, %r18, %r19;
	cvt.u32.u16	%r21, %rs6;
	shl.b32 	%r22, %r21, 16;
	cvt.u32.u16	%r23, %rs14;
	or.b32  	%r24, %r22, %r23;
	cvt.u32.u16	%r25, %rs7;
	shl.b32 	%r26, %r25, 16;
	cvt.u32.u16	%r27, %rs15;
	or.b32  	%r28, %r26, %r27;
	cvt.u32.u16	%r29, %rs8;
	shl.b32 	%r30, %r29, 16;
	cvt.u32.u16	%r31, %rs16;
	or.b32  	%r32, %r30, %r31;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	st.param.v4.b32	[func_retval0+16], {%r20, %r24, %r28, %r32};
	ret;
}

	.weak	_Z8upsampleDv16_sDv16_t
.func  (.param .align 64 .b8 func_retval0[64]) _Z8upsampleDv16_sDv16_t(
	.param .align 32 .b8 _Z8upsampleDv16_sDv16_t_param_0[32],
	.param .align 32 .b8 _Z8upsampleDv16_sDv16_t_param_1[32]
)
{
	.reg .s16 	%rs<33>;
	.reg .s32 	%r<65>;

	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv16_sDv16_t_param_0+24];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv16_sDv16_t_param_0+16];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv16_sDv16_t_param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv16_sDv16_t_param_0];
	ld.param.v4.u16 	{%rs29, %rs30, %rs31, %rs32}, [_Z8upsampleDv16_sDv16_t_param_1+24];
	ld.param.v4.u16 	{%rs25, %rs26, %rs27, %rs28}, [_Z8upsampleDv16_sDv16_t_param_1+16];
	ld.param.v4.u16 	{%rs21, %rs22, %rs23, %rs24}, [_Z8upsampleDv16_sDv16_t_param_1+8];
	ld.param.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [_Z8upsampleDv16_sDv16_t_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs17;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs18;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs19;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs20;
	or.b32  	%r16, %r14, %r15;
	cvt.u32.u16	%r17, %rs5;
	shl.b32 	%r18, %r17, 16;
	cvt.u32.u16	%r19, %rs21;
	or.b32  	%r20, %r18, %r19;
	cvt.u32.u16	%r21, %rs6;
	shl.b32 	%r22, %r21, 16;
	cvt.u32.u16	%r23, %rs22;
	or.b32  	%r24, %r22, %r23;
	cvt.u32.u16	%r25, %rs7;
	shl.b32 	%r26, %r25, 16;
	cvt.u32.u16	%r27, %rs23;
	or.b32  	%r28, %r26, %r27;
	cvt.u32.u16	%r29, %rs8;
	shl.b32 	%r30, %r29, 16;
	cvt.u32.u16	%r31, %rs24;
	or.b32  	%r32, %r30, %r31;
	cvt.u32.u16	%r33, %rs9;
	shl.b32 	%r34, %r33, 16;
	cvt.u32.u16	%r35, %rs25;
	or.b32  	%r36, %r34, %r35;
	cvt.u32.u16	%r37, %rs10;
	shl.b32 	%r38, %r37, 16;
	cvt.u32.u16	%r39, %rs26;
	or.b32  	%r40, %r38, %r39;
	cvt.u32.u16	%r41, %rs11;
	shl.b32 	%r42, %r41, 16;
	cvt.u32.u16	%r43, %rs27;
	or.b32  	%r44, %r42, %r43;
	cvt.u32.u16	%r45, %rs12;
	shl.b32 	%r46, %r45, 16;
	cvt.u32.u16	%r47, %rs28;
	or.b32  	%r48, %r46, %r47;
	cvt.u32.u16	%r49, %rs13;
	shl.b32 	%r50, %r49, 16;
	cvt.u32.u16	%r51, %rs29;
	or.b32  	%r52, %r50, %r51;
	cvt.u32.u16	%r53, %rs14;
	shl.b32 	%r54, %r53, 16;
	cvt.u32.u16	%r55, %rs30;
	or.b32  	%r56, %r54, %r55;
	cvt.u32.u16	%r57, %rs15;
	shl.b32 	%r58, %r57, 16;
	cvt.u32.u16	%r59, %rs31;
	or.b32  	%r60, %r58, %r59;
	cvt.u32.u16	%r61, %rs16;
	shl.b32 	%r62, %r61, 16;
	cvt.u32.u16	%r63, %rs32;
	or.b32  	%r64, %r62, %r63;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	st.param.v4.b32	[func_retval0+16], {%r20, %r24, %r28, %r32};
	st.param.v4.b32	[func_retval0+32], {%r36, %r40, %r44, %r48};
	st.param.v4.b32	[func_retval0+48], {%r52, %r56, %r60, %r64};
	ret;
}

	.weak	_Z8upsamplett
.func  (.param .b32 func_retval0) _Z8upsamplett(
	.param .b32 _Z8upsamplett_param_0,
	.param .b32 _Z8upsamplett_param_1
)
{
	.reg .s16 	%rs<3>;
	.reg .s32 	%r<5>;

	ld.param.u16 	%rs1, [_Z8upsamplett_param_0];
	ld.param.u16 	%rs2, [_Z8upsamplett_param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs2;
	or.b32  	%r4, %r2, %r3;
	st.param.b32	[func_retval0+0], %r4;
	ret;
}

	.weak	_Z8upsampleDv2_tS_
.func  (.param .align 8 .b8 func_retval0[8]) _Z8upsampleDv2_tS_(
	.param .align 4 .b8 _Z8upsampleDv2_tS__param_0[4],
	.param .align 4 .b8 _Z8upsampleDv2_tS__param_1[4]
)
{
	.reg .s16 	%rs<5>;
	.reg .s32 	%r<9>;

	ld.param.v2.u16 	{%rs1, %rs2}, [_Z8upsampleDv2_tS__param_0];
	ld.param.v2.u16 	{%rs3, %rs4}, [_Z8upsampleDv2_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs3;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs4;
	or.b32  	%r8, %r6, %r7;
	st.param.v2.b32	[func_retval0+0], {%r4, %r8};
	ret;
}

	.weak	_Z8upsampleDv3_tS_
.func  (.param .align 16 .b8 func_retval0[12]) _Z8upsampleDv3_tS_(
	.param .align 8 .b8 _Z8upsampleDv3_tS__param_0[8],
	.param .align 8 .b8 _Z8upsampleDv3_tS__param_1[8]
)
{
	.reg .s16 	%rs<9>;
	.reg .s32 	%r<14>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs7}, [_Z8upsampleDv3_tS__param_0];
	ld.param.v4.u16 	{%rs4, %rs5, %rs6, %rs8}, [_Z8upsampleDv3_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs4;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs5;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs6;
	or.b32  	%r12, %r10, %r11;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r13};
	ret;
}

	.weak	_Z8upsampleDv4_tS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv4_tS_(
	.param .align 8 .b8 _Z8upsampleDv4_tS__param_0[8],
	.param .align 8 .b8 _Z8upsampleDv4_tS__param_1[8]
)
{
	.reg .s16 	%rs<9>;
	.reg .s32 	%r<17>;

	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv4_tS__param_0];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv4_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs5;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs6;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs7;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs8;
	or.b32  	%r16, %r14, %r15;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	ret;
}

	.weak	_Z8upsampleDv8_tS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv8_tS_(
	.param .align 16 .b8 _Z8upsampleDv8_tS__param_0[16],
	.param .align 16 .b8 _Z8upsampleDv8_tS__param_1[16]
)
{
	.reg .s16 	%rs<17>;
	.reg .s32 	%r<33>;

	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv8_tS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv8_tS__param_0];
	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv8_tS__param_1+8];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv8_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs9;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs10;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs11;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs12;
	or.b32  	%r16, %r14, %r15;
	cvt.u32.u16	%r17, %rs5;
	shl.b32 	%r18, %r17, 16;
	cvt.u32.u16	%r19, %rs13;
	or.b32  	%r20, %r18, %r19;
	cvt.u32.u16	%r21, %rs6;
	shl.b32 	%r22, %r21, 16;
	cvt.u32.u16	%r23, %rs14;
	or.b32  	%r24, %r22, %r23;
	cvt.u32.u16	%r25, %rs7;
	shl.b32 	%r26, %r25, 16;
	cvt.u32.u16	%r27, %rs15;
	or.b32  	%r28, %r26, %r27;
	cvt.u32.u16	%r29, %rs8;
	shl.b32 	%r30, %r29, 16;
	cvt.u32.u16	%r31, %rs16;
	or.b32  	%r32, %r30, %r31;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	st.param.v4.b32	[func_retval0+16], {%r20, %r24, %r28, %r32};
	ret;
}

	.weak	_Z8upsampleDv16_tS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z8upsampleDv16_tS_(
	.param .align 32 .b8 _Z8upsampleDv16_tS__param_0[32],
	.param .align 32 .b8 _Z8upsampleDv16_tS__param_1[32]
)
{
	.reg .s16 	%rs<33>;
	.reg .s32 	%r<65>;

	ld.param.v4.u16 	{%rs13, %rs14, %rs15, %rs16}, [_Z8upsampleDv16_tS__param_0+24];
	ld.param.v4.u16 	{%rs9, %rs10, %rs11, %rs12}, [_Z8upsampleDv16_tS__param_0+16];
	ld.param.v4.u16 	{%rs5, %rs6, %rs7, %rs8}, [_Z8upsampleDv16_tS__param_0+8];
	ld.param.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [_Z8upsampleDv16_tS__param_0];
	ld.param.v4.u16 	{%rs29, %rs30, %rs31, %rs32}, [_Z8upsampleDv16_tS__param_1+24];
	ld.param.v4.u16 	{%rs25, %rs26, %rs27, %rs28}, [_Z8upsampleDv16_tS__param_1+16];
	ld.param.v4.u16 	{%rs21, %rs22, %rs23, %rs24}, [_Z8upsampleDv16_tS__param_1+8];
	ld.param.v4.u16 	{%rs17, %rs18, %rs19, %rs20}, [_Z8upsampleDv16_tS__param_1];
	cvt.u32.u16	%r1, %rs1;
	shl.b32 	%r2, %r1, 16;
	cvt.u32.u16	%r3, %rs17;
	or.b32  	%r4, %r2, %r3;
	cvt.u32.u16	%r5, %rs2;
	shl.b32 	%r6, %r5, 16;
	cvt.u32.u16	%r7, %rs18;
	or.b32  	%r8, %r6, %r7;
	cvt.u32.u16	%r9, %rs3;
	shl.b32 	%r10, %r9, 16;
	cvt.u32.u16	%r11, %rs19;
	or.b32  	%r12, %r10, %r11;
	cvt.u32.u16	%r13, %rs4;
	shl.b32 	%r14, %r13, 16;
	cvt.u32.u16	%r15, %rs20;
	or.b32  	%r16, %r14, %r15;
	cvt.u32.u16	%r17, %rs5;
	shl.b32 	%r18, %r17, 16;
	cvt.u32.u16	%r19, %rs21;
	or.b32  	%r20, %r18, %r19;
	cvt.u32.u16	%r21, %rs6;
	shl.b32 	%r22, %r21, 16;
	cvt.u32.u16	%r23, %rs22;
	or.b32  	%r24, %r22, %r23;
	cvt.u32.u16	%r25, %rs7;
	shl.b32 	%r26, %r25, 16;
	cvt.u32.u16	%r27, %rs23;
	or.b32  	%r28, %r26, %r27;
	cvt.u32.u16	%r29, %rs8;
	shl.b32 	%r30, %r29, 16;
	cvt.u32.u16	%r31, %rs24;
	or.b32  	%r32, %r30, %r31;
	cvt.u32.u16	%r33, %rs9;
	shl.b32 	%r34, %r33, 16;
	cvt.u32.u16	%r35, %rs25;
	or.b32  	%r36, %r34, %r35;
	cvt.u32.u16	%r37, %rs10;
	shl.b32 	%r38, %r37, 16;
	cvt.u32.u16	%r39, %rs26;
	or.b32  	%r40, %r38, %r39;
	cvt.u32.u16	%r41, %rs11;
	shl.b32 	%r42, %r41, 16;
	cvt.u32.u16	%r43, %rs27;
	or.b32  	%r44, %r42, %r43;
	cvt.u32.u16	%r45, %rs12;
	shl.b32 	%r46, %r45, 16;
	cvt.u32.u16	%r47, %rs28;
	or.b32  	%r48, %r46, %r47;
	cvt.u32.u16	%r49, %rs13;
	shl.b32 	%r50, %r49, 16;
	cvt.u32.u16	%r51, %rs29;
	or.b32  	%r52, %r50, %r51;
	cvt.u32.u16	%r53, %rs14;
	shl.b32 	%r54, %r53, 16;
	cvt.u32.u16	%r55, %rs30;
	or.b32  	%r56, %r54, %r55;
	cvt.u32.u16	%r57, %rs15;
	shl.b32 	%r58, %r57, 16;
	cvt.u32.u16	%r59, %rs31;
	or.b32  	%r60, %r58, %r59;
	cvt.u32.u16	%r61, %rs16;
	shl.b32 	%r62, %r61, 16;
	cvt.u32.u16	%r63, %rs32;
	or.b32  	%r64, %r62, %r63;
	st.param.v4.b32	[func_retval0+0], {%r4, %r8, %r12, %r16};
	st.param.v4.b32	[func_retval0+16], {%r20, %r24, %r28, %r32};
	st.param.v4.b32	[func_retval0+32], {%r36, %r40, %r44, %r48};
	st.param.v4.b32	[func_retval0+48], {%r52, %r56, %r60, %r64};
	ret;
}

	.weak	_Z8upsampleij
.func  (.param .b64 func_retval0) _Z8upsampleij(
	.param .b32 _Z8upsampleij_param_0,
	.param .b32 _Z8upsampleij_param_1
)
{
	.reg .s32 	%r<3>;
	.reg .s64 	%rl<5>;

	ld.param.u32 	%r1, [_Z8upsampleij_param_0];
	ld.param.u32 	%r2, [_Z8upsampleij_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r2;
	or.b64  	%rl4, %rl2, %rl3;
	st.param.b64	[func_retval0+0], %rl4;
	ret;
}

	.weak	_Z8upsampleDv2_iDv2_j
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv2_iDv2_j(
	.param .align 8 .b8 _Z8upsampleDv2_iDv2_j_param_0[8],
	.param .align 8 .b8 _Z8upsampleDv2_iDv2_j_param_1[8]
)
{
	.reg .s32 	%r<5>;
	.reg .s64 	%rl<9>;

	ld.param.v2.u32 	{%r1, %r2}, [_Z8upsampleDv2_iDv2_j_param_0];
	ld.param.v2.u32 	{%r3, %r4}, [_Z8upsampleDv2_iDv2_j_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r3;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r4;
	or.b64  	%rl8, %rl6, %rl7;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	ret;
}

	.weak	_Z8upsampleDv3_iDv3_j
.func  (.param .align 32 .b8 func_retval0[24]) _Z8upsampleDv3_iDv3_j(
	.param .align 16 .b8 _Z8upsampleDv3_iDv3_j_param_0[16],
	.param .align 16 .b8 _Z8upsampleDv3_iDv3_j_param_1[16]
)
{
	.reg .s32 	%r<9>;
	.reg .s64 	%rl<14>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r7}, [_Z8upsampleDv3_iDv3_j_param_0];
	ld.param.v4.u32 	{%r4, %r5, %r6, %r8}, [_Z8upsampleDv3_iDv3_j_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r4;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r5;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r6;
	or.b64  	%rl12, %rl10, %rl11;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl13};
	ret;
}

	.weak	_Z8upsampleDv4_iDv4_j
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv4_iDv4_j(
	.param .align 16 .b8 _Z8upsampleDv4_iDv4_j_param_0[16],
	.param .align 16 .b8 _Z8upsampleDv4_iDv4_j_param_1[16]
)
{
	.reg .s32 	%r<9>;
	.reg .s64 	%rl<17>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv4_iDv4_j_param_0];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv4_iDv4_j_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r5;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r6;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r7;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r8;
	or.b64  	%rl16, %rl14, %rl15;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	ret;
}

	.weak	_Z8upsampleDv8_iDv8_j
.func  (.param .align 64 .b8 func_retval0[64]) _Z8upsampleDv8_iDv8_j(
	.param .align 32 .b8 _Z8upsampleDv8_iDv8_j_param_0[32],
	.param .align 32 .b8 _Z8upsampleDv8_iDv8_j_param_1[32]
)
{
	.reg .s32 	%r<17>;
	.reg .s64 	%rl<33>;

	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv8_iDv8_j_param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv8_iDv8_j_param_0];
	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z8upsampleDv8_iDv8_j_param_1+16];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z8upsampleDv8_iDv8_j_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r9;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r10;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r11;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r12;
	or.b64  	%rl16, %rl14, %rl15;
	cvt.u64.u32	%rl17, %r5;
	shl.b64 	%rl18, %rl17, 32;
	cvt.u64.u32	%rl19, %r13;
	or.b64  	%rl20, %rl18, %rl19;
	cvt.u64.u32	%rl21, %r6;
	shl.b64 	%rl22, %rl21, 32;
	cvt.u64.u32	%rl23, %r14;
	or.b64  	%rl24, %rl22, %rl23;
	cvt.u64.u32	%rl25, %r7;
	shl.b64 	%rl26, %rl25, 32;
	cvt.u64.u32	%rl27, %r15;
	or.b64  	%rl28, %rl26, %rl27;
	cvt.u64.u32	%rl29, %r8;
	shl.b64 	%rl30, %rl29, 32;
	cvt.u64.u32	%rl31, %r16;
	or.b64  	%rl32, %rl30, %rl31;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	st.param.v2.b64	[func_retval0+32], {%rl20, %rl24};
	st.param.v2.b64	[func_retval0+48], {%rl28, %rl32};
	ret;
}

	.weak	_Z8upsampleDv16_iDv16_j
.func  (.param .align 128 .b8 func_retval0[128]) _Z8upsampleDv16_iDv16_j(
	.param .align 64 .b8 _Z8upsampleDv16_iDv16_j_param_0[64],
	.param .align 64 .b8 _Z8upsampleDv16_iDv16_j_param_1[64]
)
{
	.reg .s32 	%r<33>;
	.reg .s64 	%rl<65>;

	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z8upsampleDv16_iDv16_j_param_0+48];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z8upsampleDv16_iDv16_j_param_0+32];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv16_iDv16_j_param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv16_iDv16_j_param_0];
	ld.param.v4.u32 	{%r29, %r30, %r31, %r32}, [_Z8upsampleDv16_iDv16_j_param_1+48];
	ld.param.v4.u32 	{%r25, %r26, %r27, %r28}, [_Z8upsampleDv16_iDv16_j_param_1+32];
	ld.param.v4.u32 	{%r21, %r22, %r23, %r24}, [_Z8upsampleDv16_iDv16_j_param_1+16];
	ld.param.v4.u32 	{%r17, %r18, %r19, %r20}, [_Z8upsampleDv16_iDv16_j_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r17;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r18;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r19;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r20;
	or.b64  	%rl16, %rl14, %rl15;
	cvt.u64.u32	%rl17, %r5;
	shl.b64 	%rl18, %rl17, 32;
	cvt.u64.u32	%rl19, %r21;
	or.b64  	%rl20, %rl18, %rl19;
	cvt.u64.u32	%rl21, %r6;
	shl.b64 	%rl22, %rl21, 32;
	cvt.u64.u32	%rl23, %r22;
	or.b64  	%rl24, %rl22, %rl23;
	cvt.u64.u32	%rl25, %r7;
	shl.b64 	%rl26, %rl25, 32;
	cvt.u64.u32	%rl27, %r23;
	or.b64  	%rl28, %rl26, %rl27;
	cvt.u64.u32	%rl29, %r8;
	shl.b64 	%rl30, %rl29, 32;
	cvt.u64.u32	%rl31, %r24;
	or.b64  	%rl32, %rl30, %rl31;
	cvt.u64.u32	%rl33, %r9;
	shl.b64 	%rl34, %rl33, 32;
	cvt.u64.u32	%rl35, %r25;
	or.b64  	%rl36, %rl34, %rl35;
	cvt.u64.u32	%rl37, %r10;
	shl.b64 	%rl38, %rl37, 32;
	cvt.u64.u32	%rl39, %r26;
	or.b64  	%rl40, %rl38, %rl39;
	cvt.u64.u32	%rl41, %r11;
	shl.b64 	%rl42, %rl41, 32;
	cvt.u64.u32	%rl43, %r27;
	or.b64  	%rl44, %rl42, %rl43;
	cvt.u64.u32	%rl45, %r12;
	shl.b64 	%rl46, %rl45, 32;
	cvt.u64.u32	%rl47, %r28;
	or.b64  	%rl48, %rl46, %rl47;
	cvt.u64.u32	%rl49, %r13;
	shl.b64 	%rl50, %rl49, 32;
	cvt.u64.u32	%rl51, %r29;
	or.b64  	%rl52, %rl50, %rl51;
	cvt.u64.u32	%rl53, %r14;
	shl.b64 	%rl54, %rl53, 32;
	cvt.u64.u32	%rl55, %r30;
	or.b64  	%rl56, %rl54, %rl55;
	cvt.u64.u32	%rl57, %r15;
	shl.b64 	%rl58, %rl57, 32;
	cvt.u64.u32	%rl59, %r31;
	or.b64  	%rl60, %rl58, %rl59;
	cvt.u64.u32	%rl61, %r16;
	shl.b64 	%rl62, %rl61, 32;
	cvt.u64.u32	%rl63, %r32;
	or.b64  	%rl64, %rl62, %rl63;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	st.param.v2.b64	[func_retval0+32], {%rl20, %rl24};
	st.param.v2.b64	[func_retval0+48], {%rl28, %rl32};
	st.param.v2.b64	[func_retval0+64], {%rl36, %rl40};
	st.param.v2.b64	[func_retval0+80], {%rl44, %rl48};
	st.param.v2.b64	[func_retval0+96], {%rl52, %rl56};
	st.param.v2.b64	[func_retval0+112], {%rl60, %rl64};
	ret;
}

	.weak	_Z8upsamplejj
.func  (.param .b64 func_retval0) _Z8upsamplejj(
	.param .b32 _Z8upsamplejj_param_0,
	.param .b32 _Z8upsamplejj_param_1
)
{
	.reg .s32 	%r<3>;
	.reg .s64 	%rl<5>;

	ld.param.u32 	%r1, [_Z8upsamplejj_param_0];
	ld.param.u32 	%r2, [_Z8upsamplejj_param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r2;
	or.b64  	%rl4, %rl2, %rl3;
	st.param.b64	[func_retval0+0], %rl4;
	ret;
}

	.weak	_Z8upsampleDv2_jS_
.func  (.param .align 16 .b8 func_retval0[16]) _Z8upsampleDv2_jS_(
	.param .align 8 .b8 _Z8upsampleDv2_jS__param_0[8],
	.param .align 8 .b8 _Z8upsampleDv2_jS__param_1[8]
)
{
	.reg .s32 	%r<5>;
	.reg .s64 	%rl<9>;

	ld.param.v2.u32 	{%r1, %r2}, [_Z8upsampleDv2_jS__param_0];
	ld.param.v2.u32 	{%r3, %r4}, [_Z8upsampleDv2_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r3;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r4;
	or.b64  	%rl8, %rl6, %rl7;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	ret;
}

	.weak	_Z8upsampleDv3_jS_
.func  (.param .align 32 .b8 func_retval0[24]) _Z8upsampleDv3_jS_(
	.param .align 16 .b8 _Z8upsampleDv3_jS__param_0[16],
	.param .align 16 .b8 _Z8upsampleDv3_jS__param_1[16]
)
{
	.reg .s32 	%r<9>;
	.reg .s64 	%rl<14>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r7}, [_Z8upsampleDv3_jS__param_0];
	ld.param.v4.u32 	{%r4, %r5, %r6, %r8}, [_Z8upsampleDv3_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r4;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r5;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r6;
	or.b64  	%rl12, %rl10, %rl11;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl13};
	ret;
}

	.weak	_Z8upsampleDv4_jS_
.func  (.param .align 32 .b8 func_retval0[32]) _Z8upsampleDv4_jS_(
	.param .align 16 .b8 _Z8upsampleDv4_jS__param_0[16],
	.param .align 16 .b8 _Z8upsampleDv4_jS__param_1[16]
)
{
	.reg .s32 	%r<9>;
	.reg .s64 	%rl<17>;

	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv4_jS__param_0];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv4_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r5;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r6;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r7;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r8;
	or.b64  	%rl16, %rl14, %rl15;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	ret;
}

	.weak	_Z8upsampleDv8_jS_
.func  (.param .align 64 .b8 func_retval0[64]) _Z8upsampleDv8_jS_(
	.param .align 32 .b8 _Z8upsampleDv8_jS__param_0[32],
	.param .align 32 .b8 _Z8upsampleDv8_jS__param_1[32]
)
{
	.reg .s32 	%r<17>;
	.reg .s64 	%rl<33>;

	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv8_jS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv8_jS__param_0];
	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z8upsampleDv8_jS__param_1+16];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z8upsampleDv8_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r9;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r10;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r11;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r12;
	or.b64  	%rl16, %rl14, %rl15;
	cvt.u64.u32	%rl17, %r5;
	shl.b64 	%rl18, %rl17, 32;
	cvt.u64.u32	%rl19, %r13;
	or.b64  	%rl20, %rl18, %rl19;
	cvt.u64.u32	%rl21, %r6;
	shl.b64 	%rl22, %rl21, 32;
	cvt.u64.u32	%rl23, %r14;
	or.b64  	%rl24, %rl22, %rl23;
	cvt.u64.u32	%rl25, %r7;
	shl.b64 	%rl26, %rl25, 32;
	cvt.u64.u32	%rl27, %r15;
	or.b64  	%rl28, %rl26, %rl27;
	cvt.u64.u32	%rl29, %r8;
	shl.b64 	%rl30, %rl29, 32;
	cvt.u64.u32	%rl31, %r16;
	or.b64  	%rl32, %rl30, %rl31;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	st.param.v2.b64	[func_retval0+32], {%rl20, %rl24};
	st.param.v2.b64	[func_retval0+48], {%rl28, %rl32};
	ret;
}

	.weak	_Z8upsampleDv16_jS_
.func  (.param .align 128 .b8 func_retval0[128]) _Z8upsampleDv16_jS_(
	.param .align 64 .b8 _Z8upsampleDv16_jS__param_0[64],
	.param .align 64 .b8 _Z8upsampleDv16_jS__param_1[64]
)
{
	.reg .s32 	%r<33>;
	.reg .s64 	%rl<65>;

	ld.param.v4.u32 	{%r13, %r14, %r15, %r16}, [_Z8upsampleDv16_jS__param_0+48];
	ld.param.v4.u32 	{%r9, %r10, %r11, %r12}, [_Z8upsampleDv16_jS__param_0+32];
	ld.param.v4.u32 	{%r5, %r6, %r7, %r8}, [_Z8upsampleDv16_jS__param_0+16];
	ld.param.v4.u32 	{%r1, %r2, %r3, %r4}, [_Z8upsampleDv16_jS__param_0];
	ld.param.v4.u32 	{%r29, %r30, %r31, %r32}, [_Z8upsampleDv16_jS__param_1+48];
	ld.param.v4.u32 	{%r25, %r26, %r27, %r28}, [_Z8upsampleDv16_jS__param_1+32];
	ld.param.v4.u32 	{%r21, %r22, %r23, %r24}, [_Z8upsampleDv16_jS__param_1+16];
	ld.param.v4.u32 	{%r17, %r18, %r19, %r20}, [_Z8upsampleDv16_jS__param_1];
	cvt.u64.u32	%rl1, %r1;
	shl.b64 	%rl2, %rl1, 32;
	cvt.u64.u32	%rl3, %r17;
	or.b64  	%rl4, %rl2, %rl3;
	cvt.u64.u32	%rl5, %r2;
	shl.b64 	%rl6, %rl5, 32;
	cvt.u64.u32	%rl7, %r18;
	or.b64  	%rl8, %rl6, %rl7;
	cvt.u64.u32	%rl9, %r3;
	shl.b64 	%rl10, %rl9, 32;
	cvt.u64.u32	%rl11, %r19;
	or.b64  	%rl12, %rl10, %rl11;
	cvt.u64.u32	%rl13, %r4;
	shl.b64 	%rl14, %rl13, 32;
	cvt.u64.u32	%rl15, %r20;
	or.b64  	%rl16, %rl14, %rl15;
	cvt.u64.u32	%rl17, %r5;
	shl.b64 	%rl18, %rl17, 32;
	cvt.u64.u32	%rl19, %r21;
	or.b64  	%rl20, %rl18, %rl19;
	cvt.u64.u32	%rl21, %r6;
	shl.b64 	%rl22, %rl21, 32;
	cvt.u64.u32	%rl23, %r22;
	or.b64  	%rl24, %rl22, %rl23;
	cvt.u64.u32	%rl25, %r7;
	shl.b64 	%rl26, %rl25, 32;
	cvt.u64.u32	%rl27, %r23;
	or.b64  	%rl28, %rl26, %rl27;
	cvt.u64.u32	%rl29, %r8;
	shl.b64 	%rl30, %rl29, 32;
	cvt.u64.u32	%rl31, %r24;
	or.b64  	%rl32, %rl30, %rl31;
	cvt.u64.u32	%rl33, %r9;
	shl.b64 	%rl34, %rl33, 32;
	cvt.u64.u32	%rl35, %r25;
	or.b64  	%rl36, %rl34, %rl35;
	cvt.u64.u32	%rl37, %r10;
	shl.b64 	%rl38, %rl37, 32;
	cvt.u64.u32	%rl39, %r26;
	or.b64  	%rl40, %rl38, %rl39;
	cvt.u64.u32	%rl41, %r11;
	shl.b64 	%rl42, %rl41, 32;
	cvt.u64.u32	%rl43, %r27;
	or.b64  	%rl44, %rl42, %rl43;
	cvt.u64.u32	%rl45, %r12;
	shl.b64 	%rl46, %rl45, 32;
	cvt.u64.u32	%rl47, %r28;
	or.b64  	%rl48, %rl46, %rl47;
	cvt.u64.u32	%rl49, %r13;
	shl.b64 	%rl50, %rl49, 32;
	cvt.u64.u32	%rl51, %r29;
	or.b64  	%rl52, %rl50, %rl51;
	cvt.u64.u32	%rl53, %r14;
	shl.b64 	%rl54, %rl53, 32;
	cvt.u64.u32	%rl55, %r30;
	or.b64  	%rl56, %rl54, %rl55;
	cvt.u64.u32	%rl57, %r15;
	shl.b64 	%rl58, %rl57, 32;
	cvt.u64.u32	%rl59, %r31;
	or.b64  	%rl60, %rl58, %rl59;
	cvt.u64.u32	%rl61, %r16;
	shl.b64 	%rl62, %rl61, 32;
	cvt.u64.u32	%rl63, %r32;
	or.b64  	%rl64, %rl62, %rl63;
	st.param.v2.b64	[func_retval0+0], {%rl4, %rl8};
	st.param.v2.b64	[func_retval0+16], {%rl12, %rl16};
	st.param.v2.b64	[func_retval0+32], {%rl20, %rl24};
	st.param.v2.b64	[func_retval0+48], {%rl28, %rl32};
	st.param.v2.b64	[func_retval0+64], {%rl36, %rl40};
	st.param.v2.b64	[func_retval0+80], {%rl44, %rl48};
	st.param.v2.b64	[func_retval0+96], {%rl52, %rl56};
	st.param.v2.b64	[func_retval0+112], {%rl60, %rl64};
	ret;
}

	// .globl	get_ids
.visible .entry get_ids(
	.param .u32 get_ids_param_0
)
{
	.reg .s32 	%r<8>;

	ld.param.u32 	%r1, [get_ids_param_0];
	mov.u32	%r2, %ctaid.x;
	mov.u32	%r3, %ntid.x;
	mov.u32	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r2, %r4;
	shl.b32 	%r6, %r5, 2;
	add.s32 	%r7, %r1, %r6;
	st.global.u32 	[%r7], %r5;
	ret;
}

